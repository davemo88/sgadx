\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}

\usepackage{geometry}
\geometry{a4paper}
 
\usepackage{float}
\restylefloat{table}
\usepackage[english]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{amsfonts}
\usepackage{amsmath}

\usepackage{bm}

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}

\usepackage{todonotes}
\usepackage{hyperref}

% \usepackage{tikz}
% \usetikzlibrary{trees}

\title{Master's Thesis: Ads via Signaling Games}

\author{David Kasofsky}

\date{May 1, 2016}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

The goal of this paper is to demonstrate how game theory and machine learning can be applied to advertising. In particular, we model advertising interaction via signaling games and imagine the players as machine learning algorithms. Advertising is an appealing domain as it is a driving force in the success of the internet and motivates many challenging computer science problems.

\section{Advertising}
As the internet becomes the primary ad medium\cite{iab1}, advertising strategy, measurement, and implementation increasingly rely on big data, machine learning, and cloud computing. However twenty first century advertising has also motivated concerns about privacy.

Much of the activity on the internet generates data potentially useful in advertising strategy, e.g. website analytics or user demographic data. There is an incentive to store an enormous amount of data for use by advertisers. In turn, advertisers need data storage and processing frameworks which are suited to this setting, e.g. MapReduce\cite{mapreduce1}. All of this new data has generated interest in techniques that can capitalize on it, e.g. targeted advertising \cite{displayadsml1}, ad evaluation \cite{abhishek2012media}, and real-time bidding algorithms \cite{yuan2014survey}.

There are two privacy concerns here. The first is that businesses may be observing and storing personal data that the consumer does not wish to be shared or stored. The second is that businesses may treat the consumer differently based on personal data to the possible detriment of the consumer. Consumers can be manipulated into making certain decisions \cite{akerlof2015phishing} and thus there is a balance to be struck between advertiser collection and exploitation of consumer data and consumer privacy.

\todo{too abrupt need to extend}
Real-time bidding and personalized advertising are great examples of techniques in common use today. Advertisers use consumers' personal data in determining bid amounts in ad auctions as well as specific ad content to be shown. The main idea of this paper is to model personalized advertising as a game played by machine learning algorithms.

\section{Signaling Games}

A Signaling Game is a two-player game of information asymmetry. Signaling games appear in many interesting places: in mate selection by Grafen\cite{grafen1}, in job markets by Spence\cite{spence1973job}, in bargaining by Crawford\cite{crawford1982strategic} and in cybersecurity by Casey\cite{casey1}\cite{casey2}\cite{casey3}. Skyrms describes how signaling permeates life from bacteria up to humanity \cite{skyrms2010signals}.

A signaling game is a game of imperfect information or a Bayesian game\cite{harsanyi2004games}. Players begin the game with prior beliefs about whatever information is hidden and these priors naturally influence the players' strategic choices. In a signaling game, some information is hidden asymmetrically and so one player has an information advantage over the other. 

The informed player is called the sender $S$ and the uninformed player the receiver $R$. The sender $S$ has a secret type $y \in Y$ unknown to the receiver $R$. $S$ sends a signal $x \in X$ to $R$ who then performs an action $a \in A$. Players receive payoffs depending on the action $a$, signal $x$, and secret type $y$. This statement of the game is inspired by Sobel\cite{sobel1}.

\subsection{In the Wild}

Signaling games have been proposed as models in many disciplines. In 1973 Spence wrote about signaling games in the hiring process\cite{spence1973job}. Employers want to pay salaries commensurate with applicant skill and so applicants want to appear as skillful as possible. Applicant skill is the secret type $y$, the signal $x$ is the application, and the employer's offer is the action $a$. Spence concluded that deception by the applicant must be costly for the expected dynamics to obtain.

Suppose each of $y, x, a$ lie in $[0,1]$. Then we could say $\mu_{\text{applicant}}(y, x, a)$ is decreasing in $x$. This means it is harder to produce a stronger application. The key would be so that $\mu_{\text{applicant}}$ decreases faster for small $y$ than large, i.e. it is more difficult for weak applicants to produce strong applications. This is known as costly signaling and we employ it in our ad signaling game later on.

About the same time a biologist named Zahavi \cite{zahavi1} formulated the hypothesis that animal handicaps like deer antlers and peacock plumage could be explained because fitter animals are better able to endure such handicaps. Zahavi mentioned this in the context of mate selection, suggesting that animals use handicaps to communicate fitness to potential mates.

Grafen\cite{grafen1} later interpreted Zahavi's so-called handicap principle using game theory in the language of Smith\cite{smith1982evolution}. He reasoned that imposition of the handicap principle does not preclude the existence of evolutionarily stable signaling equilibrium in biological signaling systems. These biological signaling systems are signaling games, e.g. the secret type $y$ is the fitness of the animal, the message $x$ its handicap, the action $a$ is mate selection. An evolutionarily stable strategy is a refinement of Nash equilibrium strategy.

Crawford and Sobel\cite{crawford1982strategic} present a clean and mathematical description of signaling games motivated by economic bargaining. The authors start with the concept of Bayesian Nash Equilibrium, an extension of Nash equilibrium to games of imperfect information, as well as a notion of learning in repeated games with Bayes' Rule. It is exciting to see learning appear here as we wish to examine signaling games through the lens of machine learning theory.

Casey\cite{casey1} uses signaling games to model management of insiders threats in organizations. Here the sender $S$ is a member of the organization, e.g. an employee, and the receiver is the organization itself. In large organizations it is prohibitively difficult and expensive to monitor the entire potential vulnerability surface. Thus organizations must find ways to minimize their insider security risk without auditing every member of the organization.

Here the secret type $y$ is the member's status: either compliant $y_C$ or deceptive $y_D$. A deceptive member is the aforementioned insider threat. Each member sends a signal indicating his type, either $x_C$ or $x_D$. The organization must then choose to trust $a_C$ or audit $a_D$. If the member is compliant, then the organization should trust and if the member is deceptive then the organization should audit. However auditing compliant members is more costly than trusting them and so the organization cannot simply audit every member.

These examples highlight the expressivity and potential depth of signaling game models. They show how signaling games can describe situations in which privacy, trust, deception, and negotiation are the most salient features. From here we give a mathematical formulation of signaling games and give a simple example of our own.

\subsection{Definition of Signaling Game}
To define a signaling game we specify the following:
\begin{itemize}
    \item a type set $Y$ of possible sender types
    % \item distribution $D_T$ determining sender type $y \in T$
    % \item receiver's prior distribution $\pi_R$ over $T$
    \item signal set $X$ of possible signals
    \item action set $A$ of possible actions
    \item sender and receiver utility functions $\mu_S, \mu_R: Y \times X \times A \longrightarrow \mathbb{R}$
\end{itemize}

\noindent Let $a_y^* = \underset{a \in A}{\text{max }} \mu_S(y, \cdot, a)$, i.e. the action with maximum payoff to $S$ given the type $y$. Similarly let $y_a^* = \underset{y \in T}{\text{max }} \mu_R(y, \cdot, a)$, i.e. the type with the maximum payoff to $R$ given the action $a$. One could think of $a_y^*$ as the action $S$ wishes he could get $R$ to perform if $S$ of type $y$ and $y_a^*$ is the type $R$ hopes $S$ if $R$ performs action $a$.

Define the relations $f_S: T \rightarrow A$ and $f_R: A \rightarrow T$ as:
\begin{align*}
    f_S(y) = a_y^*;\\
    f_R(a) = y_a^*
\end{align*}

The message $x$ in a signaling game conveys information about the secret type $y$. For example, suppose $\vert X \vert < \vert T \vert$. In this case it is not possible for $S$ to send a unique message for each type. Conversely $R$ cannot always deduce the type from the message and therefore cannot always perform the optimal action. When a message allows $R$ to deduce $y$, the information contained in the message is maximized. Furthermore there is no deception, i.e. $R$ can choose $a$ such that $f_R(a) = y$.

However things are not quite so simple as the sender may wish to be deceptive. For instance, suppose $y \ne f_R(a_y^*)$. In other words $S$ benefits most from an action that is less desirable for $R$. In this case $S$ wants to send a message that does \emph{not} convey the true value of $y$. $S$ wishes $R$ would believe $S$ is actually type $y^\prime = f_R(a_y^*)$. The degree to which $S$ is successful is doing this related to the amount of deception in the game.

In Casey\cite{casey1} we find the following interpretation of utility functions in signaling games motivated by rate-distortion functions in information theory.  The utility functions are given as functions of the form:
\begin{align*}
    \mu_S(y, x, a) = I_S(y, x) - \lambda_S d_S(f_S(y), a)\\
    \mu_R(y, x, a) = I_R(x, a) - \lambda_R d_R(f_R(a), y)
\end{align*}
\noindent The first term in $\mu_S$ measures the amount of information conveyed by the signal about the type. The second term measures the difference between the desired action $a_y^* = f_S(y)$ and the action performed by the receiver $a$. Likewise the first term in $\mu_R$ measures the information gleaned from the signal in determining the action and the second term measures the difference between the sender's type and the ideal type $y_a^*$ given the action $a$.

Additionally Casey suggests the following measure of deception in a signaling game. Suppose the sender type $y$ is drawn according to a distribution $D_T$ over $T$. Then the receiver may have a prior $\pi_R$ over $T$, which $R$ hopes matches $D_T$. The relative entropy or KL-divergence of these distributions $D(D_T\vert\vert\pi_R)$ is a measure of deception because it compares the actual type generating distribution with the prior type distribution of $R$ and the actual type generating distribution is unknown to $R$. 

\subsection{Simple Poker}
\label{ssec:SimplePoker}

We give a toy example of a signaling game: a simplified version of two-player poker. The sender $S$ is dealt either a winning hand or a losing hand and may check or bet. The receiver $R$ then may fold or call. The payoff matrix in \autoref{simplepokerpayoffs} shows the poker-inspired messages, actions, and payoffs. Note that the game is zero-sum.

\begin{table}[H]
	\centering
	\caption{Simple Poker Payoffs}
	\label{simplepokerpayoffs}
	\begin{tabular}{ll|l|l|}
		\cline{3-4}&       & \textbf{Fold} & \textbf{Call} \\ \hline
		\multicolumn{1}{|l|}{$y=\textbf{Winner}$} & \textbf{Check} & (1,-1)  & (1,-1)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,-1)  & (2,-2)  \\ \hline
		\multicolumn{1}{|l|}{$y=\textbf{Loser}$}  & \textbf{Check} & (1,-1)  & (-1,1)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,-1)  & (-2,2) \\ \hline
	\end{tabular}
\end{table}

To use our formal definition of a signaling game, we define $\left(T, X, A, \mu_S, \mu_R, \right)$, where:
\begin{itemize}
    \item $Y = \lbrace 0, 1 \rbrace$ is the type set. To begin we chose some $y \in Y$ as the type of $S$. 0 corresponds to loser and 1 to winner.
    % \item $D_T = \lbrace 1/2,1/2 \rbrace$ is the type generating distribution.
    \item $X = \lbrace 0, 1 \rbrace$ is the signal set. 0 corresponds to check and 1 to bet.
    \item $A = \lbrace 0, 1 \rbrace$ is the action set. 0 corresponds to fold and 1 to call.
    % \item $\pi_R = \lbrace 1/2,1/2 \rbrace$ is the type prior distribution.
    \item $\mu_S: Y \times X \times A \longrightarrow \mathbb{R}$ is the sender's utility function:
        \begin{align*}
\mu_S(y, x, a) = y(1+xa) + (1-y)(1-xa-2a)
        \end{align*}
    \item $\mu_R: Y \times X \times A \longrightarrow \mathbb{R}$ is the receiver's utility function (game is zero sum):
        \begin{align*}
\mu_R(y, x, a) = -\mu_S(y, x, a)
        \end{align*}
\end{itemize}

Let $H$ be set of mixed strategies for $S$ and $G$ the set of mixed strategies for $R$. A Nash equilibrium is a pair of mixed strategies $(h,g)$ such that for all $h^\prime \in H$ and $g^\prime \in G$
\begin{align*}
    \mu_S(h^\prime,g) \le \mu_S(h,g) \land
    \mu_R(h,g^\prime) \le \mu_R(h,g)
\end{align*}

A bet by the sender signals a winner and a check signals a loser. The receiver should fold if the sender has a winner and call if the sender has a loser. However the sender can bluff by betting with a loser and so there is an opportunity for deception.

Suppose $S$ has a winning hand. Then bet dominates check for $S$ and fold dominates call for $R$. When $S$ has a losing hand then the opposite is true. Say $S$ bets all winners and checks all losers. Call this strategy $h_1$. If $S$ plays $h_1$, then $R$'s best response is to fold to all bets and call all checks ($g_1$). However $(h_1, g_1)$ is clearly not a Nash equilibrium of the game since $S$ has a different best response to $g_1$: bet every hand ($h_2$).

In considering $R$'s best response to $h_2$, we stumble across several interesting points. The first is that if $S$ bets every hand, i.e. the sender always sends the same signal regardless of the secret type, then $R$ cannot infer any information about the secret type from the signal. If this is the case, then $R$ has to make a guess about the sender's type $y$. For instance $R$ may guess that $S$ has a winning hand half the time, i.e. $P(y = \textbf{Winner}) = \frac{1}{2}$. Then $R$ could randomize uniformly between calling and folding ($g_2$). If the guess is correct then $(h_2, g_2)$ is a Nash equilibrium.



We are particularly interested in how players can learn to play equilibrium strategies in repeated signaling games. First we note that signaling games are not zero-sum in general and so the standard minimax theorem of von Neumann does not apply. In order to make satisfying progress in this vein we look to machine learning theory. In particular, we consider online learning, learning with expert advice, and regret minimization algorithms.

\section{Signaling Games in Advertising}

As mentioned, advertisers may tailor ad content for a specific consumer using that consumer's personal data. Here we cast this scenario as a signaling game. The advertiser is the sender and the consumer is the receiver. The sender's type describes the products that advertiser promotes. The message represents ad content. The action is the consumer's interaction with the ad.

We imagine ourselves in a common web scenario. A consumer visits a web page on which an advertiser may display an ad. The advertiser uses whatever data is available about the consumer to choose an ad to be displayed. The consumer clicks on the ad if they find it appealing. In this game both sender and receiver have types: the sender's corresponding to the advertiser's products and the receiver's corresponding to the consumer's interests.

The advertiser wants the consumer to click on the ad. The consumer, however, only wants to click on ads that suit their interests. This gives the advertiser an incentive to send an ad which appeals to consumer's type rather than represent their own type. This creates the potential for deception in the game.

Let us give a more formal statement. First we have the sender $S = (y_S, \pi_S, X, \mu_S, H)$, where:
\begin{itemize}
    \item $y_S \in Y_S = \mathbb{R}^D$. $y_S$ is the type of $S$. $Y_S$ is the type space of $S$.
    % \item $\pi_S$ is a probability distribution over $Y_R$, the type space of $R$.
    \item Signal space $X = \lbrace x_1, ..., x_n \vert x_i \in \mathbb{R}^D \rbrace$ .
    \item Utility function $\mu_S: Y_S \times X \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{equation}
        \mu_S = \langle y_S, x \rangle + \lambda_S a \langle x, y_R \rangle
    \end{equation}
\end{itemize}

\noindent Next we have the receiver $R = (y_R, \pi_R, A, \mu_R, G)$, where:
\begin{itemize}
    \item $y_R \in Y_R = \mathbb{R}^D$.
    \item $\pi_R(\cdot|x)$ is a conditional distribution over $Y_S$.
    \item $A = \lbrace 0,1 \rbrace$. 0 corresponds to no click, 1 to click.
    \item Utility function $\mu_R: Y_S \times Y_R \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{equation}
        \mu_R = \langle y_R, x \rangle + \lambda_R a \langle x, y_S \rangle
    \end{equation}
\end{itemize}

\noindent The first twist in this game is that both $S$ and $R$ have types $y_S$ and $y_R$ respectively. These types are $D$-dimensional vectors. Each component of these vectors can be interpreted as an affinity for a product category. We can compare the types of $S$ and $R$ to determine the suitability of the advertiser's products for the consumer. Similarly both $S$ and $R$ have priors $\pi_S$ and $\pi_R$ over the other player's type space.

Furthermore, the ad $x$ is also a $D$-dimensional vector. Once again we can interpret the components as product category affinities and imagine that the ad promotes certain types of products. The action $a$ is binary, corresponding to whether or not the consumer clicks on the ad.

The sender's utility function $\mu_S$ punishes advertisers for deception, i.e. the advertiser incurs a cost as the ad differs from the advertiser's type. If the consumer clicks, the advertiser is rewarded for matching the consumer's type with their ad. 

The receiver's utility function $\mu_R$ rewards the receiver for clicking on ads for advertisers with suitable types.

Loss in the iterated game is swap regret after $T$ rounds. Want to consider both bandit and non-bandit setting. 

\todo{code this up and have players play use the swamp regret min algo. Also expand this to population of players?}

\section{Learning and Games}

Learning theory is an appealing lens through which to examine games. It is natural to improve at a game as one plays and observes. Following this experience, we imagine the players of our signaling games as learning algorithms. The data these algorithms learn from is the history of games they have played. 

Specifically, we look to learning theory for direction in how players can learn to play equilibrium strategies in repeated signaling games. We begin with a review of online learning and learning with expert advice. This will bring us to regret minimization and how regret minimization can lead to equilibrium strategies in games. Finally we discuss a refinement of regret minimization suitable for signaling games and the types of equilibrium we can hope to achieve.

\subsection{Online Learning}

In the online learning setting, the learning algorithm is given one data point at a time over the course of $T$ rounds. Most importantly we make no distributional assumption about the data. This motivates a worst case adversarial analysis suited to the study of games. In each round $t$, the algorithm receives a point $x_t$ and predicts a label $\hat y_t$. The predicted label is compared with the true label $y_t$ and the algorithm incurs a loss according to the accuracy of its prediction.

We can concisely present the standard online learning setting as follows:

\begin{itemize}
\item for $t = 1$ to $T$:
    \begin{itemize}
    \item receive $x_t$
    \item predict $\hat{y}_t = h(x_t; w_t)$
    \item receive $y_t$
    \item incur loss $L(\hat{y}_t, y_t)$
    \item update state $w_{t+1}$ from $w_t$
    \end{itemize}
\end{itemize}
\todo{use algorithm package to make this look normal}

Our goal is to minimize total loss $\mathcal{L}_T = \sum_{t=1}^T L(\hat{y}_t,y_t)$. The method to update the state $w_t+1$ after incuring the loss for a training point is the critical part of an online learning algorithm. 

\subsection{Learning with Expert Advice}

Learning with expert advice is a classic scenario in machine learning. For simplicity take the example of binary classification, i.e. we are given input ${x_t}$ and we must output a binary label $\hat{y}_t \in \lbrace 0,1 \rbrace$. We are given a set of $N$ experts $H = \lbrace h_1,...,h_N \rbrace $ who each predict a label $\lbrace \hat y_{t,1},...,\hat y_{t,N} \rbrace$. Our task is to accurately classify the inputs using predictions offered by the experts.

The experts need not be literal experts. In the case of games, we imagine the moves available to the players as our experts. A learning algorithm then produces a distribution overs the moves, i.e. a mixed strategy. As the number of rounds increases, we would like the mixed strategies produced by the algorithms to converge to equilibrium strategies. In our example of Simple Poker, which is zero-sum, we want these to be Nash equilibrium strategies.

\subsection{Regret}

with distributional assumption we want good generalization error. however no distributional assumption here so regret + reference hypothesis. bewm

Instead of minimizing the total loss, we will compare our performance with that of the best expert in hindsight $h^*$, i.e. $h^* = \underset{i}{\text{min }} \sum_{t=1}^T L(\hat y_{t,i}, y_t)$. This is in the spirit of our worst case analysis. Since we do not have a notion of generalization in the adversarial case, it makes no sense to seek a bound on the generalization error. We instead compare our performance to a reference hypothesis. Our minimization objective then becomes

\begin{align*}
\mathcal{L}_T - \mathcal{L}_T^{\text{min}} = \sum_{t=1}^T L(\hat y_t, y_t) - \underset{i}{\text{min }} \sum_{t=1}^T L(\hat y_{t,i}, y_t)
\end{align*}

This quantity is called the external regret or simply regret $R_T$ after $T$ rounds. We are truly interested in the average regret per round $R_T/T$. If can we devise an algorithm with regret sublinear in $T$, then the average regret $R_T/T$ will asymptotically go to 0. Such an algorithm is said to have vanishing regret.

Critically, there is a connection between regret minimization and repeated games. First we consider the simplest case of the two-player zero-sum game, e.g. our game of simple poker. If a player plays using a regret minimization algorithm with vanishing regret then that player's expected payoff per round will approach the value of the game, i.e. the minimax payoff (See \textbf{Theorem 7.2} in \cite{cesa2006prediction}).

\subsection{Weighted Majority}

The Weighted Majority algorithm \autoref{wmalgo} \cite{littlestone1994weighted} is a classic example of a regret minimization algorithm. Initially each expert is given equal weight. In each round we predict according to the weighted majority of the experts, e.g. in the first round this will be the usual majority. Whenever our prediction is incorrect, we reduce the weight of each expert who predicted incorrectly in that round. We do so by multiplying its current weight $w_{t,i}$ by some constant $\beta$ with $0 \le \beta < 1$. As an expert makes errors, its weight decreases and so its prediction counts less towards the ultimate prediction made by the algorithm.

\begin{algorithm}[H]
\caption{Weighted Majority}
\label{wmalgo}
\begin{algorithmic}[1]
\For{$i=1$ to $N$}
    \State{$w_{1,i} \leftarrow \frac{1}{N}$}
\EndFor
\For{$t=1$ to $T$}
    \State \text{receive } $x_t , y_t$
    \State $\hat y_t \leftarrow 1_{\sum_{i, y_{t,i}=1}^N w_t \ge \sum_{i, y_{t,i}=0}^N w_t}$
    \If{$\hat y_t \ne y_t$}
        \For{$i=1$ to $N$}
            \If{$y_{t,i} \ne y_t$}
                \State $w_{t+1,i} \leftarrow \beta w_{t+1,i} \quad\quad(0 \le \beta < 1)$
            \Else
                \State $w_{t+1,i} \leftarrow w_{t,i}$
            \EndIf
        \EndFor
    \EndIf
\EndFor
\State \Return $\textbf{w}_{T+1}$
\end{algorithmic}
\end{algorithm}

However, recall that we are focused on the adversarial case. This algorithm (or any deterministic algorithm, for that matter) will fare poorly against an adversary. To see this, suppose our adversary chooses data points for which the current weighted majority prediction will be wrong. We will predict incorrectly in every round. In the case of binary classification with binary loss our cumulative loss $\mathcal{L}_T = \sum_{t=1}^T R_t$ will be at least $T/2$ and so the algorithm has constant regret per round.

The Randomized Weighted Majority algorithm\autoref{rwmalgo} alleviates this by making probabilistic predictions. Instead of predicting according to the weighted majority, we sample from a distribution over the experts and predict according to the chosen expert. This distribution is determined by the experts' weights which are updated in the same fashion as the original Weighed Majority aglorhithm. Of course the adversary can still frustrate us to some degree but we shall see that this relatively simple modification to the algorithm produces a surprisingly promising result.

\begin{algorithm}[H]
\caption{Randomized Weighted Majority}
\label{rwmalgo}
\begin{algorithmic}[1]
\For{$i=1$ to $N$}
    \State{$w_{1,i} \leftarrow 1$}
    \State{$p_{1,i} \leftarrow \frac{1}{N}$}
\EndFor
\For{$t=1$ to $T$}
    \State \text{receive } $x_t, y_t$
    \For{$i=1$ to $N$}
        \If{$\hat y_{t,i} \ne y_t$}
            \State $w_{t+1,i} \leftarrow \beta w_{t+1,i} \quad\quad(0 \le \beta < 1)$
        \Else
            \State $w_{t+1,i} \leftarrow w_{t,i}$
        \EndIf
    \EndFor
    \State $W_{t+1} \leftarrow \sum_{i=1}^N w_{t+1}$
    \For{$i=1$ to $N$}
        \State $p_{t,i} = w_{t+1, i}/W_{t+1}$
    \EndFor
\EndFor
\State \Return $\textbf{w}_{T+1}$
\end{algorithmic}
\end{algorithm}
This algorithm obeys the following standard regret bound\cite{mohri2012foundations} for $\beta = \text{max}(\lbrace 1/2, 1-\sqrt{(\log N) T}\rbrace)$
\begin{align*}
R_T = \mathcal{L}_T - \mathcal{L}_T^{\text{min}} \le 2\sqrt{T \log N}
\end{align*}
To prove this, we consider the potential function
\begin{align*}
W_{t} = \sum_{i=1}^N w_{t,i}
\end{align*}
\noindent and give upper and lower bounds. For the upper bound, we note that $W_{t+1}$ can be decomposed into two sums: one of the weights of experts who predicted correctly in round $t$ and the other of $\beta$ times the weights of the experts who predicted incorrectly. Let $L_t$ be the loss in round $t$. With some manipulation we can express $W_{t+1}$ in terms of $W_{t}$ and $L_t$ as:
\begin{align*}
W_{t+1} = W_t(1-(1-\beta)L_t)
\end{align*}
\noindent Recall $w_{1,i} = 1$ for all $i$ and so $W_1 = N$. Therefore:
\begin{align*}
W_{T+1} = N \prod_{t=1}^T (1-(1-\beta)L_t)
\end{align*}
\noindent For the lower bound, we give an easy one: $W_{T+1} \ge \underset{i}{\text{max }} w_{T+1,i}=\beta^{\mathcal{L_T^\text{min}}}$, where $\mathcal{L_T^\text{min}}$ is the loss of the best single expert. In the case of binary classification and binary loss $\mathcal{L_T^\text{min}}$ is equal to the number of mistakes made by the best expert.

Now we compare our bounds:
\begin{align*}
    \beta^\mathcal{L_T^\text{min}} \le N \prod_{t=1}^T (1-(1-\beta)L_t)
\end{align*}
\noindent we take the $\log$ and apply the inequality $\log(1-x) \le -x$ valid for $x<1$:
\begin{align*}
    \mathcal{L_T^\text{min}}\log\beta & \le \log\left(N \prod_{t=1}^T (1-(1-\beta)L_t)\right)\\
    & \le \log N - (1-\beta)\sum_{t=1}^T L_t\\
    & = \log N - (1-\beta) \mathcal{L}_t\\
\end{align*}
\begin{align*}
\mathcal{L}_T \le \frac{\log N}{1-\beta} + (2-\beta)\mathcal{L_T^\text{min}}
\end{align*}
\noindent In the case of the binary loss we know that $\mathcal{L_T^\text{min}} \le T$ so we can upper bound one more time:
\begin{align*}
\mathcal{L}_T \le \frac{\log N}{1-\beta} + (1-\beta)T + \mathcal{L_T^\text{min}}
\end{align*}
\noindent Now we differentiate this upper bound with respect to $\beta$ and set it to zero to find the optimal value. This gives us $\frac{\log N}{(1-\beta)^2} = 0$ which we can solve to find $\beta = \beta_0 = 1 - \sqrt{(\log N)\ T}$. Thus $\beta_0$ is the optimizing value if $1 - \sqrt{(\log N)\ T} \ge 1/2$ and $1/2$ is otherwise.

This regret bound is sublinear in $T$ and so this algorithm has vanishing regret, i.e. $\lim_{t\rightarrow\infty} R_T/T = 0$. As per the theorem from Cesa-Bianchi\cite{cesa2006prediction}, we may conclude that mixed strategies generated by Randomized Weighted Majority can approximate Nash equilibrium strategies in two-player zero-sum games given large enough $T$. While this is encouraging, it is not the case that a regret minimization algorithm can generate Nash equilibrium strategies for general games, e.g. non-zero-sum games. 

In fact finding Nash equilibrium in general games is known to be intractible (PPAD-complete\cite{daskalakis2009complexity}). PPAD problems are distinct from NP problems in that we always know that a solution exists but these solutions may be very hard to find. Specifically, PPAD problems are concerned with polynomial time verification of a binary relation $r(x,y)$ for which it is known that for all $x$ there exists a $y$ such that $(x,y)$ holds. For example we always know that a Nash equilibrium exists for any given game. A related example of a PPAD problem is finding a fixed point of a continuous function over a compact convex set, as in Brouwer's fixed point theorem which inspired Nash.

\subsection{Correlated Equilibrium}

Signaling games are not necessarily zero sum and so Nash equilibrium is not a computationally attractive solution. Instead we turn to another sort of equilibrium called correlated equilibrium. Originally introduced by Aumann \cite{aumann1974subjectivity}\cite{aumann1987correlated}, correlated equilibrium is different from Nash in that it specifies a joint distribution over the actions of all players rather than individual distributions over the actions of each player. Let $H$ be the set of mixed strategies for $S$ and $G$ the set of mixed strategies for $R$. A correlated equilibrium is a joint distribution $p$ over $G \times H$ such that for all $h\in H$ with $p(h) > 0$ and all $h^\prime \in H$
\begin{align*}
\sum_{g \in G} p(h,g)\mu_S(h, g) \ge \sum_{g \in G} p(h^\prime,g)\mu_S(h^\prime, g)
\end{align*}
\noindent and similarly for $R$
\begin{align*}
\sum_{h \in H} p(h,g)\mu_R(h, g) \ge \sum_{h \in H} p(h,g^\prime)\mu_R(h, g^\prime)
\end{align*}

The intuition behind correlated equilibrium is that players may rationally play strategies besides Nash equilibrium strategies when all players observe the value of a random variable, sometimes called a correlation device. The device chooses joint strategy assignments for each player with some probability and informs each player of their chosen strategy. If no player has an incentive to deviate from the assigned strategy (considering the known distribution over joint strategy assignments) then the joint distribution is a correlated equilibrium.

First, we note that an explicit correlation device is not necessary to achieve correlated equilibrium in repeated games. Foster and Vohra \cite{foster1997calibrated} show that players who play myopic best responses to calibrated forecasts of the other player's strategies will converge to a correlated equilibrium. A calibrated forecast and the concept of calibration come from statistics. It roughly means that players assign probabilities to each other's actions that coincide with the actions' empirical probabilities in the limit. Here the history of plays can be viewed as an implicit correlation device.

\subsection{Internal Regret and Swap Regret}

As external regret minimization algorithms achieve Nash equilibrium in zero-sum games, swap regret minimization algorithms achieve correlated equilibrium in general games \cite{blum2007external}. First we introduce the concept of \emph{internal regret} \cite{foster1998asymptotic}.
\begin{align*}
\sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(a_t)] - \underset{f \in C_{a,b}}{\text{min }} \sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(f(a_t))]
\end{align*}
Instead of using the best single expert in hindsight as our reference, we compare our performance to the ways in which we could have exchanged a single action for another. $C_{a,b}$ is the set of functions $f: A \rightarrow A$ that perform this single action exchange.

Hart and Mas-Colell\cite{hart2000simple} give a method for playing repeated games in which the players update their strategies in proportion to internal regret. At each step $t$ each player sticks with his last move $j$ or switches to a different one $k$ with some probability. We compare the cumulative loss of the sequence formed by replacing $j$ with $k$ in a player's sequence of moves thus far with the actual cumulative loss $\mathcal{L}_t$.

Call this alternative loss $\mathcal{L}_t^{j\rightarrow k}$. As we wish to minimize internal regret, the only moves with positive probabilities are those with $\mathcal{L}_t^{j\rightarrow k} < \mathcal{L}_t$. The probability on move $k$ is then proportional to the regret for playing $j$ instead of $k$, $\mathcal{L}_t - \mathcal{L}_t^{j\rightarrow k}$. The last strategy $j$ always retains some probability mass as well.

\emph{Swap regret} considers all ways in which actions could be mapped between themselves, i.e. where we can swap as many actions as we please.
\begin{align*}
\sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(a_t)] - \underset{f \in C}{\text{min }} \sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(f(a_t))]
\end{align*}
Here the set $C$ contains all functions $f: A \rightarrow A$. Recall we can imagine the actions or experts as the moves in the game, and so having zero swap regret implies no strategic modifications would be advantageous. The joint distribution over the players' actions is a correlated equilibrium if every player would have 0 swap regret playing it. Players minimizing swap regret will thereby converge to correlated equilibrium.

Blum and Mansour\cite{blum2007external} prove this and conveniently give us a way to generate an algorithm with vanishing swap regret given an algorithm with vanishing external regret. Given such a regret minimization algorithm, say Randomized Weighted Majority, we instantiate $N$ copies, one for each expert or pure strategy. At each step each instance $i$ generates a distribution $q_i$ which will form a component of the overall mixed strategy.

When the algorithm receives a loss vector $l_t$, i.e. a vector of losses for each action, it is partitioned among the $N$ instances. Each instance $i$ receives a fraction of the loss $p_{t,i}$ equal to the probability mass instance $i$ currently puts on action $i$ with its distribution $q_i$. This means that the cumulative loss of some other action $j$ for instance $i$ is $\sum_{s=1}^t p_{t,i} l_{t,j}$. This matches the cost we could incur by putting the mass of instance $i$ on instance $j$.

\todo{give more proof details, e.g. stationary distribution \textbf{Q} etc}

\todo{discuss bandit and non-bandit settings}

The idea is that instance $i$ will ensure low regret with respect to swaps for action $i$ , i.e. of functions $f_{i,j} \in C_{i,j}$. Finally we are able to exploit the underlying external regret guarantees for each instance to give a swap regret bound

\begin{align*}
R_T^{\text{swap}} = \mathcal{L}_T - \mathcal{L}_T^{\text{swap}} \le c N\sqrt{T \log N}
\end{align*}

where $c$ is some constant. Recall that for Randomized Weighted Majority, we have that the external regret $R_T \le 2\sqrt{T \log N}$. The great part about this bound is that the dependence on $N$ is only logarithmic and so we are relatively free to consider a very large numbers of expert. In the case of swap regret we cannot maintain this luxury and suffer a linear dependence on $N$. While we are able to apply swap regret minimization to a broader class of games, we have heavier restrictions on the number of experts. It is possible to achieve an $O(\sqrt{N T \log N})$ swap regret bound but that is not much next to the $\log N$ bound.

\section{Swap Regret Minimization in Signaling Games}

Finally we return to our ad game, armed with the theory of online learning and a swap regret minimizing algorithm. 

\bibliographystyle{amsplain}
\bibliography{dk_thesis}

\end{document}
