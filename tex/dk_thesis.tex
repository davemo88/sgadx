\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}

\usepackage{geometry}
\geometry{a4paper}
 
\usepackage{float}
\restylefloat{table}
\usepackage[english]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{amsfonts}
\usepackage{amsmath}

\usepackage{bm}

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}

\usepackage{todonotes}
\usepackage{hyperref}

% \usepackage{tikz}
% \usetikzlibrary{trees}

\title{Master's Thesis: Ads via Signaling Games}

\author{David Kasofsky}

\date{May 1, 2016}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

The goal of this paper is to demonstrate how of game theory and machine learning can be applied advertising. In particular, we model advertising interaction via signaling games and imagine the players as machine learning algorithms. Advertising is an appealing domain as it is a driving force in the success of the internet and motivates many challenging computer science problems.

\section{Advertising}
As the internet becomes the primary ad medium\cite{iab1}, advertising strategy, measurement, and implementation increasingly rely on big data, machine learning, and cloud computing. However twenty first century advertising has also motivated concerns about privacy.

Much of the activity on the internet generates data potentially useful in advertising strategy, e.g. website analytics or user demographic data. There is an incentive to store an enormous amount of data for use by advertisers. In turn, advertisers need data storage and processing frameworks which are suited to this setting, e.g. MapReduce\cite{mapreduce1}. All of this new data has generated interest in techniques that can make sense of it, e.g. targeted advertising \cite{displayadsml1}, ad evaluation \cite{abhishek2012media}, and real-time bidding algorithms \cite{yuan2014survey}.

There are two privacy concerns here. The first is that businesses may be observing and storing personal data that the consumer does not wish to be shared or stored. The second is that businesses may treat the consumer differently based on personal data to the possible detriment of the consumer. Consumers can be manipulated into making certain decisions \cite{akerlof2015phishing} and thus there is a balance to be struck between advertiser collection and exploitation of consumer data and consumer privacy.

\todo{too abrupt need to extend}
Real-time bidding and personalized advertising are great examples of techniques in common use today. Advertisers use consumers' personal data in determining bid amounts in ad auctions as well as specific ad content to be shown. The main idea of this paper is to model personalized advertising as a game played by machine learning algorithms.

\section{Signaling Games}

A Signaling Game is a two-player game of information asymmetry. Signaling games appear in many interesting places: in mate selection by Grafen\cite{grafen1}, in job markets by Spence\cite{spence1973job}, in bargaining by Crawford\cite{crawford1982strategic} and in cybersecurity by Casey\cite{casey1}\cite{casey2}\cite{casey3}. Skyrms describes how signaling permeates life from bacteria up to humanity \cite{skyrms2010signals}.

A signaling game is a game of imperfect information or a Bayesian game\cite{harsanyi2004games}. Players begin the game with prior beliefs about whatever information is hidden and these priors naturally influence the players' strategic choices. In a signaling game, some information is hidden asymmetrically and so one player has an information advantage over the other. 

The informed player is called the sender $S$ and the uninformed player the receiver $R$. The sender $S$ has a secret type $\tau \in T$ unknown to the receiver $R$. $S$ sends a signal $\sigma \in \Sigma$ to $R$ who then performs an action $\alpha \in A$. Players receive payoffs depending on the action $\alpha$, signal $\sigma$, and secret type $\tau$. This statement of the game is inspired by Sobel\cite{sobel1}.

\subsection{In the Wild}

Signaling games have been proposed as models in many disciplines. In 1973 Spence wrote about signaling games in the hiring process\cite{spence1973job}. Employers want to pay salaries commensurate with applicant' skill so applicants want to appear as skillful as possible. Applicant skill is the secret type $\tau$, the signal $\sigma$ is the application, and the employer's offer is the action $\alpha$. Spence concluded that deception by the applicant must be costly for the game to be interesting.

Suppose each of $\tau, \sigma, \alpha$ lie in $[0,1]$. Then we could say $\mu_{\text{applicant}}(\tau, \sigma, \alpha)$ is decreasing in $\sigma$. The key would be so that $\mu_{\text{applicant}}$ decreases faster for small $\tau$ than large, i.e. it is more difficult for weaker applicants to produce strong applications. This is known as costly signaling and we employ it in our ad signaling game later on.

About the same time a biologist named Zahavi \cite{zahavi1} formulated the hypothesis that animal handicaps like deer antlers and peacock plumage could be explained because fitter animals are better able to endure such handicaps. Zahavi mentioned this in the context of mate selection, suggesting that animals use handicaps to communicate fitness to potential mates.

Grafen\cite{grafen1} later interpreted Zahavi's so-called handicap principle using game theory in the language of Smith\cite{smith1982evolution}. He reasoned that imposition of the handicap principle does not preclude the existence of evolutionarily stable signaling equilibrium in biological signaling systems. These biological signaling systems are signaling games, e.g. the secret type $\tau$ is the fitness of the animal and the message $\sigma$ its handicap. An evolutionarily stable strategy is a refinement of Nash equilibrium strategy.

Crawford and Sobel\cite{crawford1982strategic} present a clean and mathematical description of signaling games. The authors start with the concept of Bayesian Nash Equilibrium, an extension of Nash equilibrium to games of imperfect information, as well as a notion of learning with Bayes' Rule. It is exciting to see learning appear here as we wish to examine signaling games through the lens of machine learning theory.

Casey\cite{casey1} 

\subsection{Definition of Signaling Game}
To define a signaling game we specify the following:
\begin{itemize}
    \item a type set $T$ of possible sender types
    \item distribution $D_T$ determining sender type $\tau \in T$
    \item receiver prior distribution $\pi$ over $T$
    \item signal set $\Sigma$ of possible signals
    \item action set $A$ of possible actions
    \item sender and receiver utility functions $\mu_S, \mu_R: T \times \Sigma \times A \longrightarrow \mathbb{R}$
\end{itemize}

\noindent Let $\alpha_\tau^* = \underset{\alpha \in A}{\text{max }} \mu_S(\tau, \cdot, \alpha)$, i.e. the action with maximum payoff to $S$. Similarly let $\tau_\alpha^* = \underset{\tau \in T}{\text{max }} \mu_R(\tau, \cdot, \alpha)$, i.e. the type with the maximum payoff to $R$ given the action $\alpha$. One could think of $\alpha_\tau^*$ as the action $S$ wishes he could get $R$ to perform if $S$ of type $\tau$ and $\tau_\alpha^*$ is the type $R$ hopes $S$ if $R$ performs action $\alpha$.

Define the relations $f_S: T \rightarrow A$ and $f_R: A \rightarrow T$ as:
\begin{align*}
    f_S(\tau) = \alpha_\tau^*;\\
    f_R(\alpha) = \tau_\alpha^*
\end{align*}

% In Casey\cite{casey1} we find the following interpretation of the utility functions motivated by rate-distortion functions in information theory.  The utility functions are functions of the form:
% \begin{equation}
%     \mu_S(\tau, \sigma, \alpha) = I_S(\tau, \sigma) + \lambda_S d_S(f_S(\tau), \alpha)
% \label{sig_sender_util}
% \end{equation}\begin{equation}
%     \mu_R(\tau, \sigma, \alpha) = I_R(\sigma, \alpha) + \lambda_R d_R(f_R(\alpha), \tau)
% \label{sig_receiver_util}
% \end{equation}
% \noindent The first term in \autoref{sig_sender_util} measures the amount of information conveyed by the signal about the type. The second term measures the difference between the desired action $\alpha_\tau^* = f_S(\tau)$ and the action performed by the receiver $\alpha$. Likewise the first term in \autoref{sig_receiver_util} measures the information gleaned from the signal in determining the action and the second term measures the difference between the sender's type and the ideal type $\tau_\alpha^*$ given the action $\alpha$.

The message $\sigma$ in a signaling game conveys information about the secret type $\tau$. For example, suppose $\vert \Sigma \vert < \vert T \vert$. In this case it is not possible for $S$ to send a unique message for each type. Conversely $R$ cannot always deduce the type from the message and therefore cannot always perform the optimal action. When a message allows $R$ to deduce $\tau$, the information contained in the message is maximized. Furthermore there is no deception, i.e. $R$ can choose $\alpha$ such that $f_R(\alpha) = \tau$.

However things are not quite so simple as the sender may wish to be deceptive. For instance, suppose $\tau \ne f_R(\alpha_\tau^*)$. In other words $S$ benefits most from an action that is undesirable for $R$. In this case $S$ wants to send a message that does \emph{not} convey the true value of $\tau$. $S$ wishes $R$ would believe $S$ is actually type $\tau^\prime = f_R(\alpha_\tau^*)$. The degree to which $S$ is successful is doing this 

\subsection{Simple Poker}
\label{ssec:SimplePoker}

Here we have a toy example of a signaling game: a simplified version of two-player poker. The sender $S$ is dealt either a winning hand or a losing hand and may check or bet. The receiver $R$ then may fold or call. The payoff matrix in \autoref{simplepokerpayoffs} shows the poker-inspired messages, actions, and payoffs.

\begin{table}[H]
	\centering
	\caption{Simple Poker Payoffs}
	\label{simplepokerpayoffs}
	\begin{tabular}{ll|l|l|}
		\cline{3-4}&       & \textbf{Fold} & \textbf{Call} \\ \hline
		\multicolumn{1}{|l|}{$\tau=\textbf{Winner}$} & \textbf{Check} & (1,-1)  & (1,-1)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,-1)  & (2,-2)  \\ \hline
		\multicolumn{1}{|l|}{$\tau=\textbf{Loser}$}  & \textbf{Check} & (1,-1)  & (-1,1)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,-1)  & (-2,2) \\ \hline
	\end{tabular}
\end{table}

For a more formal statement of the game, we define $\left(T, \Sigma, A, \mu_S, \mu_R, \right)$, where:
\begin{itemize}
    \item $T = \lbrace 0, 1 \rbrace$ is the type set. To begin we chose some $\tau \in T$ as the type of $S$. 0 corresponds to loser and 1 to winner.
    \item $\Sigma = \lbrace 0, 1 \rbrace$ is the signal set. 0 corresponds to check and 1 to bet.
        \item $A = \lbrace 0, 1 \rbrace$ is the action set. 0 corresponds to fold and 1 to call.
    \item $\mu_S: T \times \Sigma \times A \longrightarrow \mathbb{R}$ is the sender's utility function:
        \begin{align*}
\mu_S(\tau, \sigma, \alpha) = \tau(1+\sigma\alpha) + (1-\tau)(1-\sigma\alpha-\alpha)s
        \end{align*}
    \item $\mu_R: T \times \Sigma \times A \longrightarrow \mathbb{R}$ is the receiver's utility function:
        \begin{align*}
\mu_R(\tau, \sigma, \alpha) = \tau(-\sigma\alpha) + (1-\tau)(\sigma\alpha+\alpha)
        \end{align*}
\end{itemize}

Let $H$ be set of mixed strategies for $S$ and $G$ the set of mixed strategies for $R$. A Nash equilibrium is a pair of mixed strategies $(h,g)$ such that for all $h^\prime \in H$ and $g^\prime \in G$
\begin{align*}
    \mu_S(h^\prime,g) \le \mu_S(h,g) \land
    \mu_R(h,g^\prime) \le \mu_R(h,g)
\end{align*}

A bet by the sender signals a winner and a check signals a loser. The receiver should fold if the sender has a winner and call if the sender has a loser. However the sender can bluff by betting with a loser and so there is an opportunity for deception.

Suppose $S$ has a winning hand. Then bet dominates check for $S$ and fold dominates call for $R$. When $S$ has a losing hand then the opposite is true. Say $S$ bets all winners and checks all losers. Call this strategy $h_1$. If $S$ plays $h_1$, then $R$'s best response is to fold to all bets and call all checks ($g_1$). However $(h_1, g_1)$ is clearly not a Nash equilibrium of the game since $S$ has a different best response to $g_1$: bet every hand ($h_2$).

In considering $R$'s best response to $h_2$, we stumble across several interesting points. The first is that if $S$ bets every hand, i.e. the sender always sends the same signal regardless of the secret type, then $R$ cannot infer any information about the secret type from the signal. If this is the case, then $R$ has to make a guess about the sender's type $\tau$. For instance $R$ may guess that $S$ has a winning hand half the time, i.e. $P(\tau = \textbf{Winner}) = \frac{1}{2}$. Then $R$ could randomize uniformly between calling and folding ($g_2$). If the guess is correct then $(h_2, g_2)$ is a Nash equilibrium.

We are particularly interested in how players can learn to play equilibrium strategies in repeated signaling games. First we note that signaling games are not zero-sum in general and so the standard minimax theorem of von Neumann does not apply. In order to make satisfying progress in this vein we look to machine learning theory. In particular, we consider online learning, learning with expert advice, and regret minimization algorithms.

\section{Signaling Games in Advertising}

As mentioned, advertisers may tailor ad content for a specific consumer using that consumer's personal data. Here we cast this scenario as a signaling game. The advertiser is the sender and the consumer is the receiver. The sender's type describes the products that advertiser promotes. The message represents ad content. The action is the consumer's interaction with the ad.

We imagine ourselves in a common web scenario. A consumer visits a web page on which an advertiser may display an ad. The advertiser uses whatever data is available about the consumer to choose an ad to be displayed. The consumer clicks on the ad if they find it appealing. In this game both sender and receiver have types: the sender's corresponding to the advertiser's products and the receiver's corresponding to the consumer's interests.

The advertiser wants the consumer to click on the ad. The consumer, however, only wants to click on ads that suit their interests. This gives the advertiser an incentive to send an ad which appeals to consumer's type rather than represent their own type. This creates the potential for deception in the game.

Let us give a more formal statement. First we have the sender $S = (\tau_S, \pi_S, \Sigma, \mu_S, H)$, where:
\begin{itemize}
    \item $\tau_S \in T_S = \mathbb{R}^D$. $\tau_S$ is the type of $S$. $T_S$ is the type space of $S$.
    \item $\pi_S$ is a probability distribution over $T_R$, the type space of $R$.
    \item Signal space $\Sigma = \lbrace \sigma_1, ..., \sigma_n \vert \sigma_i \in \mathbb{R}^D \rbrace$ .
    \item Utility function $\mu_S: T_S \times \Sigma \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{equation}
        \mu_S = \langle \tau_S, \sigma \rangle + \lambda_S \alpha \langle \sigma, \tau_R \rangle
    \end{equation}
\end{itemize}

\noindent Next we have the receiver $R = (\tau_R, \pi_R, A, \mu_R, G)$, where:
\begin{itemize}
    \item $\tau_R \in T_R = \mathbb{R}^D$.
    \item $\pi_R(\cdot|\sigma)$ is a conditional distribution over $T_S$.
    \item $A = \lbrace 0,1 \rbrace$. 0 corresponds to no click, 1 to click.
    \item Utility function $\mu_R: T_S \times T_R \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{equation}
        \mu_R = \langle \tau_R, \sigma \rangle + \lambda_R \alpha \langle \sigma, \tau_S \rangle
    \end{equation}
\end{itemize}

\noindent The first twist in this game is that both $S$ and $R$ have types $\tau_S$ and $\tau_R$ respectively. These types are $D$-dimensional vectors. Each component of these vectors can be interpreted as an affinity for a product category. We can compare the types of $S$ and $R$ to determine the suitability of the advertiser's products for the consumer. Similarly both $S$ and $R$ have priors $\pi_S$ and $\pi_R$ over the other player's type space.

Furthermore, the ad $\sigma$ is also a $D$-dimensional vector. Once again we can interpret the components as product category affinities and imagine that the ad promotes certain types of products. The action $\alpha$ is binary, corresponding to whether or not the consumer clicks on the ad.

The sender's utility function $\mu_S$ punishes advertisers for deception, i.e. the advertiser incurs a cost as the ad differs from the advertiser's type. If the consumer clicks, the advertiser is rewarded for matching the consumer's type with their ad. 

The receiver's utility function $\mu_R$ rewards the receiver for clicking on ads for advertisers with suitable types.

Loss in the iterated game is swap regret after $T$ rounds. Want to consider both bandit and non-bandit setting. 

\todo{code this up and have players play use the swamp regret min algo. Also expand this to population of players?}

\section{Learning and Games}

Learning theory is an appealing lens through which to examine games. It is natural to improve at a game as one plays and observes. Following this experience, we imagine the players of our signaling games as learning algorithms. The data these algorithms learn from is the history of games they have played. 

Specifically, we look to learning theory for direction in how players can learn to play equilibrium strategies in repeated signaling games. We begin with a review of online learning and learning with expert advice. This will bring us to regret minimization and how regret minimization can lead to equilibrium strategies in games. Finally we discuss a refinement of regret minimization suitable for signaling games and the types of equilibrium we can hope to achieve.

\subsection{Online Learning}

In the online learning setting, the learning algorithm is given one data point at a time over the course of $T$ rounds. Most importantly we make no distributional assumption about the data. This motivates a worst case adversarial analysis suited to the study of games. In each round $t$, the algorithm receives a point $x_t$ and predicts a label $\hat y_t$. The predicted label is compared with the true label $y_t$ and the algorithm incurs a loss according to the accuracy of its prediction.

We can concisely present the standard online learning setting as follows:

\begin{itemize}
\item for $t = 1$ to $T$:
    \begin{itemize}
    \item receive $x_t$
    \item predict $\hat{y}_t = h(x_t; w_t)$
    \item receive $y_t$
    \item incur loss $L(\hat{y}_t, y_t)$
    \item update state $w_{t+1}$ from $w_t$
    \end{itemize}
\end{itemize}
\todo{use algorithm package to make this look normal}

Our goal is to minimize total loss $\mathcal{L}_T = \sum_{t=1}^T L(\hat{y}_t,y_t)$. The method to update the state $w_t+1$ after incuring the loss for a training point is the critical part of an online learning algorithm. 

\subsection{Learning with Expert Advice}

Learning with expert advice is a classic scenario in machine learning. For simplicity take the example of binary classification, i.e. we are given input ${x_t}$ and we must output a binary label $\hat{y}_t \in \lbrace 0,1 \rbrace$. We are given a set of $N$ experts $H = \lbrace h_1,...,h_N \rbrace $ who each predict a label $\lbrace \hat y_{t,1},...,\hat y_{t,N} \rbrace$. Our task is to accurately classify the inputs using predictions offered by the experts.

The experts need not be literal experts. In the case of games, we imagine the moves available to the players as our experts. A learning algorithm then produces a distribution overs the moves, i.e. a mixed strategy. As the number of rounds increases, we would like the mixed strategies produced by the algorithms to converge to equilibrium strategies. In our example of Simple Poker, which is zero-sum, we want this to be these to be Nash equilibrium strategies.

\subsection{Regret}

with distributional assumption we want good generalization error. however no distributional assumption here so regret + reference hypothesis. bewm

Instead of minimizing the total loss, we will compare our performance with that of the best expert in hindsight $h^*$, i.e. $h^* = \underset{i}{\text{min }} \sum_{t=1}^T L(\hat y_{t,i}, y_t)$. This is in the spirit of our worst case analysis. Since we do not have a notion of generalization in the adversarial case, it makes no sense to seek a bound on the generalization error. We instead compare our performance to a reference hypothesis. Our minimization objective then becomes

\begin{align*}
\mathcal{L}_T - \mathcal{L}_T^{\text{min}} = \sum_{t=1}^T L(\hat y_t, y_t) - \underset{i}{\text{min }} \sum_{t=1}^T L(\hat y_{t,i}, y_t)
\end{align*}

This quantity is called the external regret or simply regret $R_T$ after $T$ rounds. We are truly interested in the average regret per round $R_T/T$. If can we devise an algorithm with regret sublinear in $T$, then the average regret $R_T/T$ will go to 0. Such an algorithm is said to have vanishing regret.

Critically, there is a connection between regret minimization and repeated games. First we consider the simplest case of the two-player zero-sum game, e.g. our game of simple poker. If a player plays using a regret minimization algorithm with vanishing regret then that player's expected payoff per round will approach the value of the game, i.e. the minimax payoff (See \textbf{Theorem 7.2} in \cite{cesa2006prediction}).

\subsection{Weighted Majority}

The Weighted Majority algorithm \autoref{wmalgo} \cite{littlestone1994weighted} is a classic example of a regret minimization algorithm. Initially each expert is given equal weight. In each round we predict according to the weighted majority of the experts, e.g. in the first round this will be the usual majority. Whenever our prediction is incorrect, we reduce the weight of each expert who predicted incorrectly in that round. As an expert makes errors, its weight decreases and so its prediction counts less towards the overall prediction made by the algorithm.

\begin{algorithm}[H]
\caption{Weighted Majority}
\label{wmalgo}
\begin{algorithmic}[1]
\For{$i=1$ to $N$}
    \State{$w_{1,i} \leftarrow \frac{1}{N}$}
\EndFor
\For{$t=1$ to $T$}
    \State \text{receive } $x_t , y_t$
    \State $\hat y_t \leftarrow 1_{\sum_{i, y_{t,i}=1}^N w_t \ge \sum_{i, y_{t,i}=0}^N w_t}$
    \If{$\hat y_t \ne y_t$}
        \For{$i=1$ to $N$}
            \If{$y_{t,i} \ne y_t$}
                \State $w_{t+1,i} \leftarrow \beta w_{t+1,i}$
            \Else
                \State $w_{t+1,i} \leftarrow w_{t,i}$
            \EndIf
        \EndFor
    \EndIf
\EndFor
\State \Return $\textbf{w}_{T+1}$
\end{algorithmic}
\end{algorithm}

However, recall that we are focused on the adversarial case. This algorithm (or any deterministic algorithm, for that matter) will fare poorly against an adversary. To see this, suppose our adversary chooses data points for which the current weighted majority prediction will be wrong. We will predict incorrectly in every round. In the case of the binary loss our regret $R_T$ will be at least $T/2$ and so the algorithm has constant regret per round.

The Randomized Weighted Majority algorithm\autoref{rwmalgo} alleviates this by making probabilistic predictions. Of course the adversary can still frustrate us to some degree but we shall see that this relatively simple modification to the algorithm produces a surprisingly promising result.

\begin{algorithm}[H]
\caption{Randomized Weighted Majority}
\label{rwmalgo}
\begin{algorithmic}[1]
\For{$i=1$ to $N$}
    \State{$w_{1,i} \leftarrow 1$}
    \State{$p_{1,i} \leftarrow \frac{1}{N}$}
\EndFor
\For{$t=1$ to $T$}
    \State \text{receive } $x_t, y_t$
    \For{$i=1$ to $N$}
        \If{$\hat y_{t,i} \ne y_t$}
            \State $w_{t+1,i} \leftarrow \beta w_{t+1,i} \quad\quad(0 \le \beta < 1)$
        \Else
            \State $w_{t+1,i} \leftarrow w_{t,i}$
        \EndIf
    \EndFor
    \State $W_{t+1} \leftarrow \sum_{i=1}^N w_{t+1}$
    \For{$i=1$ to $N$}
        \State $p_{t,i} = w_{t+1, i}/W_{t+1}$
    \EndFor
\EndFor
\State \Return $\textbf{w}_{T+1}$
\end{algorithmic}
\end{algorithm}

This algorithm obeys the following standard regret bound \cite{mohri2012foundations}

\begin{align*}
R_T = \mathcal{L}_T - \mathcal{L}_T^{\text{min}} \le 2\sqrt{T \log N}
\end{align*}

This regret bound is sublinear in $T$ and so this algorithm has vanishing regret. This means that mixed strategies generated by Randomized Weighted Majority can approximate Nash equilibrium strategies in two-player zero-sum games given large enough $T$. While this is encouraging, it is not the case that a regret minimization algorithm can generate Nash equilibrium strategies for general games, e.g. non-zero-sum games. In fact finding Nash equilibrium in general games is known to be intractible (PPAD-complete\cite{daskalakis2009complexity}).

\subsection{Correlated Equilibrium}

\todo{discuss bandit and non-bandit settings}

Signaling games are not necessarily zero sum and so Nash equilibrium is not a computationally attractive solution. Instead we turn to another sort of equilibrium called correlated equilibrium. Originally introduced by Aumann \cite{aumann1974subjectivity}\cite{aumann1987correlated}, correlated equilibrium is different from Nash in that it specifies a joint distribution over the actions of all players rather than individual distributions over the actions of each player. Let $H$ be the set of mixed strategies for $S$ and $G$ the set of mixed strategies for $R$. A correlated equilibrium is a joint distribution $p$ over $G \times H$ such that for all $h\in H$ with $p(h) \> 0$ and all $h^\prime \in H$
\begin{align*}
\sum_{g \in G} p(h,g)\mu_S(h, g) \ge \sum_{g \in G} p(h^\prime,g)\mu_S(h^\prime, g)
\end{align*}
\noindent and similarly for $R$
\begin{align*}
\sum_{h \in H} p(h,g)\mu_R(h, g) \ge \sum_{h \in H} p(h,g^\prime)\mu_R(h, g^\prime)
\end{align*}

The intuition behind correlated equilibrium is that players may rationally play strategies besides Nash equilibrium strategies when all players observe the value of a random variable, sometimes called a correlation device. The device chooses joint strategy assignments for each player with some probability and informs each player of their chosen strategy. If no player has an incentive to deviate from the assigned strategy (considering the known distribution over joint strategy assignments) then the joint distribution is a correlated equilibrium.

First, we note that an explicit correlation device is not necessary to achieve correlated equilibrium in repeated games. Foster and Vohra \cite{foster1997calibrated} show that players who play myopic best responses to calibrated forecasts of the other player's strategies will converge to a correlated equilibrium. A calibrated forecast and the concept of calibration come from statistics. It roughly means that players assign probabilities to each other's actions that coincide with the actions' empirical probabilities in the limit. Here the history of plays can be viewed as an implicit correlation device.

\subsection{Internal Regret and Swap Regret}

As external regret minimization algorithms achieve Nash equilibrium in zero-sum games, swap regret minimization algorithms achieve correlated equilibrium in general games \cite{blum2007external}. First we introduce the concept of \emph{internal regret} \cite{foster1998asymptotic}.
\begin{align*}
\sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(a_t)] - \underset{f \in C_{a,b}}{\text{min }} \sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(f(a_t))]
\end{align*}
Instead of using the best single expert in hindsight as our reference, we compare our performance to the ways in which we could have exchanged a single action for another. $C_{a,b}$ is the set of functions $f: A \rightarrow A$ that perform this single action exchange.

Hart and Mas-Colell\cite{hart2000simple} give a method for playing repeated games in which the players update their strategies in proportion according to internal regret. Each player maintains a distribution over the available actions. Suppose at time $t$ player $i$ plays action $k$. $U_t$ is the cumulative payoff so far. Now for each other action $j$, $V_t(k)$ is what the cumulative payoff would have been if $i$ had always done $k$ instead of $j$. Only actions with $V_t(k) > U_t$ are appealing. The probability mass for each such $k$ is then updated by an amount proportional to the regret term $V_t(k) - U_t$. One could think of $U_t$ as the current reference point of $i$ from which he evaluates potential modifications to his choices in hindsight.

\emph{Swap regret} considers all ways in which actions could be mapped between themselves, i.e. where we can swap as many actions as we please.
\begin{align*}
\sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(a_t)] - \underset{f \in C}{\text{min }} \sum_{t=1}^T \underset{a_t \sim p_t}{E}[l(f(a_t))]
\end{align*}
Here the set $C$ contains all functions $f: A \rightarrow A$. Recall we can imagine the actions or experts as the moves in the game, and so having zero swap regret implies no strategic modifications would be advantageous. The joint distribution over the players' actions is a correlated equilibrium if every player would have 0 swap regret playing it. Players minimizing swap regret will thereby converge to correlated equilibrium.

Blum and Mansour\cite{blum2007external} prove this and conveniently give us a way to generate an algorithm with vanishing swap regret given an algorithm with vanishing external regret. Given such a regret minimization algorithm, say Randomized Weighted Majority, we instantiate $N$ copies, one for each expert or pure strategy. At each step each instance $i$ generates a distribution $q_i$ which will form a component of the overall mixed strategy.

When the algorithm receives a loss vector $l_t$, i.e. a vector of losses for each action, it is partitioned among the $N$ instances. Each instance $i$ receives a fraction of the loss $p_{t,i}$ equal to the probability mass instance $i$ currently puts on action $i$ with its distribution $q_i$. This means that the cumulative loss of some other action $j$ for instance $i$ is $\sum_{s=1}^t p_{t,i} l_{t,j}$. This matches the cost we could incur by putting the mass of instance $i$ on instance $j$.

\todo{give more proof details, e.g. stationary distribution \textbf{Q} etc}

The idea is that instance $i$ will ensure low regret with respect to swaps for action $i$ , i.e. of functions $f_{i,j} \in C_{i,j}$. Finally we are able to exploit the underlying external regret guarantees for each instance to give a swap regret bound

\begin{align*}
R_T^{\text{swap}} = \mathcal{L}_T - \mathcal{L}_T^{\text{swap}} \le c N\sqrt{T \log N}
\end{align*}

where $c$ is some constant. Recall that for Randomized Weighted Majority, we have that the external regret $R_T \le 2\sqrt{T \log N}$. The great part about this bound is that the dependence on $N$ is only logarithmic and so we are relatively free to consider a very large numbers of expert. In the case of swap regret we cannot maintain this luxury and suffer a linear dependence on $N$. While we are able to apply swap regret minimization to a broader class of games, we have heavier restrictions on the number of experts. It is possible to achieve an $O(\sqrt{N T \log N})$ swap regret bound but that is not much next to the $\log N$ bound.

\section{Swap Regret Minimization in Signaling Games}

Finally we return to our ad game, armed with the theory of online learning and a swap regret minimizing algorithm. 

\bibliographystyle{amsplain}
\bibliography{dk_thesis}

\section{Appendix}

\end{document}
