\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}

\usepackage{geometry}
\geometry{a4paper}
 
\usepackage{float}
\restylefloat{table}

\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[noend]{algpseudocode}

\usepackage[english]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{amsfonts}
\usepackage{amsmath}

\usepackage{bm}

\usepackage{tikz}
\usetikzlibrary{trees}

\title{Signaling Game Ad Exchange}

\author{David Kasofsky}

\date{May 1, 2016}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

TODO

\newpage

\section{Themes of Online Advertising}

\subsection{online advertising as a mascot for technological challenge}
\subsection{online advertising as a land of secret types}
\subsection{consumer data / third party data}
\subsection{spam and fraud}
\subsection{game theory}

\section{Signaling Games}

A Signaling Game is a two-player game of information asymmetry. The sender $S$ has a secret type $\tau \in T$ unknown to the receiver $R$, who merely has a prior $\pi$ over $T$. $S$ sends a signal $\sigma$ to $R$ who then performs an action $\alpha$. Players receive payoffs depending on the action $\alpha$, signal $\sigma$, and secret type $\tau$. This statement of the game is inspired by Sobel\cite{sobel1}.

Signaling games appear in many interesting places: in mate selection by Zahavi\cite{zahavi1} and Grafen\cite{grafen1}, in job markets by Spence[3], in TODO by Crawford\cite{crawford1} and in cybersecurity by Casey\cite{casey1}\cite{casey2}\cite{casey3}. To build our intuition, we will start with an example from Poker.

\subsection{Simple Poker}

Poker is a great example of a signaling game because it neatly displays the depth of strategic interaction with asymmetric information. It showcases a hallmark of signaling games: deception.

Consider a simplified version of two-player poker. The sender $S$ is dealt either a winning hand or a losing hand and may signal bet or check. The receiver $R$ then may call or fold. The payoff matrix in \ref{simplepokerpayoffs} shows the poker-inspired messages, actions, and payoffs.

\begin{table}[H]
	\centering
	\caption{Simple Poker Payoffs}
	\label{simplepokerpayoffs}
	\begin{tabular}{ll|l|l|}
		\cline{3-4}
		&       & \textbf{Fold} & \textbf{Call} \\ \hline
		\multicolumn{1}{|l|}{\textbf{Winner}} & \textbf{Check} & (1,0)  & (1,0)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,0)  & (2,-1)  \\ \hline
		\multicolumn{1}{|l|}{\textbf{Loser}}  & \textbf{Check} & (1,0)  & (0,1)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,0)  & (-1,2) \\ \hline
	\end{tabular}
\end{table}

Suppose $S$ has a winning hand. Then bet dominates check. However if $S$ has a losing hand then check dominates bet. So say $S$ bets all winners and checks all losers. Call this strategy $h_1$. If $S$ plays $h_1$, then $R$'s best response is to fold to all bets and call all checks ($g_1$). However $(h_1, g_1)$ is clearly not a Nash equilibrium of the game since $S$ has a different best response to $g_1$: bet every hand ($h_2$).

In considering $R$'s best response to $h_2$, we stumble across several interesting points. The first is that if $S$ bets every hand, i.e. the sender always sends the same signal regardless of the secret type, then $R$ cannot infer any information about the secret type from the signal. If this is the case, then $R$ can only rely on prior beliefs about the secret type. For instance $R$ may believe that $S$ has a winning hand half the time. Then $R$ could randomize uniformly between calling and folding ($g_2$).

Second, poker is usually played as a repeated game. If we adopt that here, then the receiver may use Bayesian inference to update prior beliefs about the sender's type. The Nash equilibrium of the repeated game will depend on the true probability $p$ that $S$ has a winning hand. For example, if $p = \frac{1}{2}$ then $(h_2, g_2)$ is a Nash equilibrium.

To conclude our discussion of this simple game, let us give it in somewhat more formal language. First we define the sender $S = (t, \Sigma, \mu_S, H)$, where:
\begin{itemize}
	\item $\tau \in T = \lbrace 0, 1 \rbrace$ is the type of $S$. 0 corresponds to loser and 1 to winner.
	\item $\Sigma = \lbrace 0, 1 \rbrace$ is the set of signals. 0 corresponds to check and 1 to bet.
	\item $\mu_S: T \times \Sigma \times A \longrightarrow \mathbb{R}$ is defined as:
		\begin{equation}
\mu_S(\tau, \sigma, \alpha) = \tau(1+\sigma\alpha) + (1-\tau)(1-\sigma\alpha-\alpha)
		\end{equation}
	\item $H = \lbrace h | h: \tau \rightarrow \Sigma \rbrace$ is the sender's strategy set.
\end{itemize}

\noindent Next we have the receiver $R = (\pi, A, \mu_R, G)$, where:
\begin{itemize}
	\item $\pi(\cdot|\sigma) \in \Pi$ is a conditional distribution over $T$ given $\sigma \in \Sigma$.
	\item $A = \lbrace 0, 1 \rbrace$ is the set of actions. 0 corresponds to fold and 1 to call.
	\item $\mu_R: T \times \Sigma \times A \longrightarrow \mathbb{R}$ is defined as:
		\begin{equation}
\mu_R(\tau, \sigma, \alpha) = \tau(-\sigma\alpha) + (1-\tau)(\sigma\alpha+\alpha)
		\end{equation}
	\item $G = \lbrace g | g: \Sigma \times \Pi \rightarrow A \rbrace$ is the receiver's strategy set.
\end{itemize}

\noindent These are the utility functions used to generate \ref{simplepokerpayoffs}. We introduce the strategy sets $H$ and $G$ to naturally constrain the complexity of strategies which can be used.

NOTE: I am trying to add this here because I want to be able to talk about finding solutions for games like I talk about straightforward machine learning problems, e.g. training and testing with examples, imagining repeated games as online learning problems, and using gradient based optimization. So I want parameterized strategies and I want differentiable utility functions.

Finally we give the iterated version of the game.

\begin{algorithm}
	\caption{Simple Poker}\label{simplepokeralgo}
	\begin{algorithmic}[1]
	\STATE $\pi_0 \leftarrow \pi$
	\FOR{$t = 1$ to $\eta$}
		\STATE $\tau \leftarrow sample(T)$
		\STATE $\sigma \leftarrow h(\tau)$
		\STATE $\alpha \leftarrow g(\sigma, \pi)$
		\STATE $\pi_{t+1}(\cdot|\sigma) \leftarrow BayesianUpdate(\pi_t(\cdot|\sigma))$
		\IF{$\tau = \alpha$} \STATE something \ENDIF
	\ENDFOR
	\end{algorithmic}
\end{algorithm}

\newpage

\section{recommenders \& verifiers}

online prediction with expert advice

look at online learning lecture: \emph{http://www.cims.nyu.edu/~mohri/amls/online\_learning\_basics.pdf}

look at randomized weighted majority algorithm

\section{Online Advertising via Signaling Games}

In this twist on the signaling game, we depict basic online advertising interaction. The sender is an advertiser and the receiver a consumer. The advertiser sends ads to consumer

Let $U^D$ be the set of unit vectors in dimension $D$.

First we have the sender $S = (\tau_S, \pi_S, \Sigma, \mu_S, H)$, where:
\begin{itemize}
	\item $\tau_S \in T_S = U^D$. $\tau_S$ is the type of $S$. $T_S$ is the type space of $S$.
	\item $\pi_S$ is a probability distribution over $T_R$, the type space of $R$.
	\item Signal space $\Sigma = U^D$.
	\item Utility function $\mu_S: T_S \times \Sigma \times A \rightarrow \mathbb{R}$ is defined as:
	\begin{equation}
		\mu_S(\tau_S, \delta, \alpha) = \langle \tau_S, \delta \rangle + \alpha
	\end{equation}
	\item Strategy set $H$ is defined as:
	\begin{equation}
		H = \lbrace h | h(\tau_S, \pi_S; \lambda) = \frac{\lambda \tau_S + (1-\lambda)\underset{x\sim\pi_S}{E}\lbrack x \rbrack}{Z}; 0 \le \lambda \le 1 \rbrace
	\end{equation}
\end{itemize}

\noindent $S$ is limited by the strategy set $H$ to signals which are a normalized convex combination of $\tau_S$ and $\underset{x\sim\pi_S}{E}\lbrack x \rbrack$, the average type according to the prior $\pi_S$.

QUESTION: what is corresponding rotation instead of the normalized convex combination?

\noindent Next we have the receiver $R = (\tau_R, \pi_R, A, \mu_R, G)$, where:
\begin{itemize}
	\item $\tau_R \in T_R = U^D$.
	\item $\pi_R(\cdot|\sigma)$ is a conditional distribution over $T_S$.
	\item $A = \lbrace 0,1 \rbrace$. 0 corresponds to no click, 1 to click.
	\item Utility function $\mu_R: T_S \times T_R \times A \rightarrow \mathbb{R}$ is defined as:
	\begin{equation}
		\mu_R(\tau_S, \tau_R, \alpha) = \alpha \langle \tau_S, \tau_R \rangle
	\end{equation}
	\item Strategy set $G$ is defined as:
	\begin{equation}
		G = \lbrace g | g(\tau_R, \delta; \theta) = 1_{\langle \tau_R, \delta \rangle > \theta}; 0 \le \theta \le 1 \rbrace
	\end{equation}
\end{itemize}

\noindent $R$ is limited by the strategy set $G$ to threshold functions of the inner product $\langle \tau_R, \delta \rangle$.

In the iterated game, each player wants to tune their parameter ($\lambda$ for senders and $\theta$ for receivers) to be optimal against the population of opposing players. The simulation will be a sort of parallel stochastic gradient descent in which players are randomly paired to play against each other, execute their current strategy, then receive their payoff. Payoff functions are public knowledge so players are able to make inferences about the distribution of types in the population and thereby pick a direction and learning rate for gradient descent.

Loss in the iterated game is average regret after $\eta$ rounds. We expose the types of the players when we give payoffs but not their identity, i.e. players cannot recognize each other after meeting.

%TODO: extend $\lambda$ and $\theta$ to higher dimension somehow so we end up with many parameters and so have gradient descent method that uses prior for direction and learning rate

\section{References}

\bibliographystyle{amsplain}
\bibliography{dk_thesis}

\end{document}
