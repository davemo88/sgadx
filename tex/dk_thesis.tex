\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}

\usepackage{geometry}
\geometry{a4paper}
 
\usepackage{float}
\restylefloat{table}
\usepackage[english]{babel}

\usepackage{setspace}
\doublespacing

\usepackage{amsfonts}
\usepackage{amsmath}

\usepackage{bm}

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}

\usepackage{todonotes}
\usepackage{hyperref}

% \usepackage{tikz}
% \usetikzlibrary{trees}

\title{Master's Thesis: Ads via Signaling Games}

\author{David Kasofsky}

\date{May 1, 2016}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

The goal of this paper is to demonstrate how game theory and machine learning can be applied to advertising. In particular, we model advertising interaction via signaling games and imagine the players as machine learning algorithms. Advertising is an appealing domain as it is a driving force in the success of the internet and motivates many challenging computer science problems.

\section{Advertising}
As the internet becomes the primary ad medium\cite{iab1}, advertising strategy, measurement, and implementation increasingly rely on big data, machine learning, and cloud computing. However twenty first century advertising has also motivated concerns about privacy.

Much of the activity on the internet generates data potentially useful in advertising strategy, e.g. website analytics or user demographic data. There is an incentive to store an enormous amount of data for use by advertisers. In turn, advertisers need data storage and processing frameworks which are suited to this setting, e.g. MapReduce\cite{mapreduce1}. All of this new data has generated interest in techniques that can capitalize on it, e.g. targeted advertising \cite{displayadsml1}, ad evaluation \cite{abhishek2012media}, and real-time bidding algorithms \cite{yuan2014survey}.

There are two privacy concerns here. The first is that businesses may be observing and storing personal data that the consumer does not wish to be shared or stored. The second is that businesses may treat the consumer differently based on personal data to the possible detriment of the consumer. Consumers can be manipulated into making certain decisions \cite{akerlof2015phishing} and thus there is a balance to be struck between advertiser collection and exploitation of consumer data and consumer privacy.

Personalized advertising is a great examples of a technique in common use today that pushes the boundaries of consumer privacy. Advertisers use consumers' personal data in determining bid amounts in ad auctions as well as specific ad content to be shown. Ultimately advertisers would like to create a totally customized experience for each consumer, tuned to that user's interests and behavior.

\section{Signaling Games}

A Signaling Game is a two-player game of information asymmetry. Signaling games appear in many interesting places: in mate selection by Grafen\cite{grafen1}, in job markets by Spence\cite{spence1973job}, in bargaining by Crawford\cite{crawford1982strategic} and in cybersecurity by Casey\cite{casey1}\cite{casey2}\cite{casey3}. Skyrms describes how signaling permeates life from bacteria up to humanity \cite{skyrms2010signals}.

A signaling game is a game of imperfect information or a Bayesian game\cite{harsanyi2004games}. Players begin the game with prior beliefs about whatever information is hidden and these priors naturally influence the players' strategic choices. In a signaling game, some information is hidden asymmetrically and so one player has an information advantage over the other. 

The informed player is called the sender $S$ and the uninformed player the receiver $R$. The sender $S$ has a secret type $y \in Y$ unknown to the receiver $R$. $S$ sends a signal $x \in X$ to $R$ who then performs an action $a \in A$. Players receive payoffs depending on the action $a$, signal $x$, and secret type $y$. This statement of the game is inspired by Sobel\cite{sobel1}.

\subsection{In the Wild}

Signaling games have been proposed as models in many disciplines. In 1973 Spence wrote about signaling games in the hiring process\cite{spence1973job}. Employers want to pay salaries commensurate with applicant skill and so applicants want to appear as skillful as possible. Applicant skill is the secret type $y$, the signal $x$ is the application, and the employer's offer is the action $a$. Spence concluded that deception by the applicant must be costly for the game to be interesting.

Suppose each of $y, x, a$ lie in $[0,1]$. Then we could say $\mu_{\text{applicant}}(y, x, a)$ is decreasing in $x$. In the job application setting this means it is harder to produce a stronger application. The key would be so that $\mu_{\text{applicant}}$ decreases faster for small $y$ than large, i.e. it is more difficult for weak applicants to produce strong applications. This is known as costly signaling and we employ it in our ad signaling game later on. Costly signaling penalizes the sender somehow in proportion to the signal, e.g. to its magnitude.

About the same time a biologist named Zahavi \cite{zahavi1} formulated the hypothesis that animal handicaps like deer antlers and peacock plumage could be explained because fitter animals are better able to endure such handicaps. Zahavi mentioned this in the context of mate selection, suggesting that animals use handicaps to communicate fitness to potential mates. Handicaps, like clumsy antlers and extravagant plumage, are costly signals.

Grafen\cite{grafen1} later interpreted Zahavi's so-called handicap principle using game theory in the language of Smith\cite{smith1982evolution}. He reasoned that imposition of the handicap principle does not preclude the existence of evolutionarily stable signaling equilibrium in biological signaling systems. These biological signaling systems are signaling games, e.g. the secret type $y$ is the fitness of the animal, the message $x$ its handicap, the action $a$ is mate selection. An evolutionarily stable strategy is a refinement of Nash equilibrium strategy.

Crawford and Sobel\cite{crawford1982strategic} present a clean and mathematical description of signaling games motivated by economic bargaining. The authors start with the concept of Bayesian Nash Equilibrium, an extension of Nash equilibrium to games of imperfect information, as well as a notion of learning in repeated games with Bayes' Rule. It is exciting to see learning appear here as we wish to examine signaling games through the lens of machine learning theory.

Casey\cite{casey1} uses signaling games to model management of insiders threats in organizations. Here the sender $S$ is a member of the organization, e.g. an employee, and the receiver is the organization itself. In large organizations it is prohibitively difficult and expensive to monitor the entire potential vulnerability surface. Thus organizations must find ways to minimize their insider security risk without constantly auditing every member of the organization.

Here the secret type $y$ is the member's status: either compliant $y_C$ or deceptive $y_D$. A deceptive member is the aforementioned insider threat. Each member sends a signal indicating its type, either $x_C$ or $x_D$. The organization must then choose to trust $a_C$ or audit $a_D$. If the member is compliant, then the organization should trust and if the member is deceptive then the organization should audit. However auditing compliant members is more costly than trusting them and so the organization cannot simply audit every member.

These examples highlight the expressivity and potential depth of signaling game models. They show how signaling games can describe situations in which privacy, trust, deception, and negotiation are the most salient features. From here we give a mathematical formulation of signaling games and give a simple example of our own.

\subsection{Mathematical Definition}
To define a signaling game we specify the following:
\begin{itemize}
    \item a type set $Y$ of possible sender types
    % \item distribution $D_T$ determining sender type $y \in T$
    % \item receiver's prior distribution $\pi_R$ over $T$
    \item signal set $X$ of possible signals
    \item action set $A$ of possible actions
    \item sender and receiver utility functions $\mu_S, \mu_R: Y \times X \times A \longrightarrow \mathbb{R}$
\end{itemize}

\noindent Let $a_y^* = \underset{a \in A}{\text{max }} \mu_S(y, \cdot, a)$, i.e. the action with maximum payoff to $S$ given the type $y$. Similarly let $y_a^* = \underset{y \in T}{\text{max }} \mu_R(y, \cdot, a)$, i.e. the type with the maximum payoff to $R$ given the action $a$. One could think of $a_y^*$ as the action $S$ wishes he could get $R$ to perform if $S$ of type $y$ and $y_a^*$ is the type $R$ hopes $S$ if $R$ performs action $a$.

Define the relations $f_S: T \rightarrow A$ and $f_R: A \rightarrow T$ as:
\begin{align*}
    f_S(y) = a_y^*;\\
    f_R(a) = y_a^*
\end{align*}

The message $x$ in a signaling game conveys information about the secret type $y$. For example, suppose $\vert X \vert < \vert T \vert$. In this case it is not possible for $S$ to send a unique message for each type. Conversely $R$ cannot always deduce the type from the message and therefore cannot always perform the optimal action. When a message allows $R$ to deduce $y$, the information contained in the message is maximized. Furthermore there is no deception, i.e. $R$ can choose $a$ such that $f_R(a) = y$.

However things are not quite so simple as the sender may wish to be deceptive. For instance, suppose $y \ne f_R(a_y^*)$. In other words $S$ benefits most from an action that is less desirable for $R$. In this case $S$ wants to send a message that does \emph{not} convey the true value of $y$. $S$ wishes $R$ would believe $S$ is actually type $y^\prime = f_R(a_y^*)$. The degree to which $S$ is successful is doing this related to the amount of deception in the game.

In Casey\cite{casey1} we find the following interpretation of utility functions in signaling games motivated by rate-distortion functions in information theory.  The utility functions are given as functions of the form:
\begin{align*}
    \mu_S(y, x, a) = I_S(y, x) - \lambda_S d_S(f_S(y), a)\\
    \mu_R(y, x, a) = I_R(x, a) - \lambda_R d_R(f_R(a), y)
\end{align*}
\noindent The first term in $\mu_S$ measures the amount of information conveyed by the signal about the type. The second term measures the difference between the desired action $a_y^* = f_S(y)$ and the action performed by the receiver $a$. Likewise the first term in $\mu_R$ measures the information gleaned from the signal in determining the action and the second term measures the difference between the sender's type and the ideal type $y_a^*$ given the action $a$.

Additionally Casey suggests the following measure of deception in a signaling game. Suppose the sender type $y$ is drawn according to a distribution $D_Y$ over $Y$. Then the receiver may have a prior $\pi_R$ over $Y$, which $R$ hopes matches $D_Y$. The relative entropy or KL-divergence of $\pi_R$ from $D_Y$, $D(D_Y\vert\vert\pi_R)$, is a measure of deception because it compares the prior type distribution of $R$ with the actual type generating distribution and the actual type generating distribution is unknown to $R$. 

\subsection{Simple Poker}
\label{ssec:SimplePoker}

We give a toy example of a signaling game: a simplified version of two-player poker. The sender $S$ is dealt either a winning hand or a losing hand and may check or bet. The receiver $R$ then may fold or call. The payoff matrix in \autoref{simplepokerpayoffs} shows the poker-inspired messages, actions, and payoffs. Note that the game is zero-sum.

\begin{table}[H]
	\centering
	\caption{Simple Poker Payoffs}
	\label{simplepokerpayoffs}
	\begin{tabular}{ll|l|l|}
		\cline{3-4}&       & \textbf{Fold} & \textbf{Call} \\ \hline
		\multicolumn{1}{|l|}{$y=\textbf{Winner}$} & \textbf{Check} & (1,-1)  & (1,-1)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,-1)  & (2,-2)  \\ \hline
		\multicolumn{1}{|l|}{$y=\textbf{Loser}$}  & \textbf{Check} & (1,-1)  & (-1,1)  \\ \cline{2-4}
		\multicolumn{1}{|l|}{}        & \textbf{Bet}   & (1,-1)  & (-2,2) \\ \hline
	\end{tabular}
\end{table}

To use our formal definition of a signaling game, we define $\left(Y, X, A, \mu_S, \mu_R, \right)$, where:
\begin{itemize}
    \item $Y = \lbrace 0, 1 \rbrace$ is the type set. To begin we chose some $y \in Y$ as the type of $S$. 0 corresponds to loser and 1 to winner.
    % \item $D_T = \lbrace 1/2,1/2 \rbrace$ is the type generating distribution.
    \item $X = \lbrace 0, 1 \rbrace$ is the signal set. 0 corresponds to check and 1 to bet.
    \item $A = \lbrace 0, 1 \rbrace$ is the action set. 0 corresponds to fold and 1 to call.
    % \item $\pi_R = \lbrace 1/2,1/2 \rbrace$ is the type prior distribution.
    \item $\mu_S: Y \times X \times A \longrightarrow \mathbb{R}$ is the sender's utility function:
        \begin{align*}
\mu_S(y, x, a) = y(1+xa) + (1-y)(1-xa-2a)
        \end{align*}
    \item $\mu_R: Y \times X \times A \longrightarrow \mathbb{R}$ is the receiver's utility function (game is zero sum):
        \begin{align*}
\mu_R(y, x, a) = -\mu_S(y, x, a)
        \end{align*}
\end{itemize}

A bet by the sender signals a winner and a check signals a loser. The receiver should fold if the sender has a winner and call if the sender has a loser. However the sender can bluff by betting with a loser and so there is an opportunity for deception.

Suppose $S$ has a winning hand. Then bet dominates check for $S$ and fold dominates call for $R$. When $S$ has a losing hand then the opposite is true. Say $S$ bets all winners and checks all losers. Call this strategy $h_1$. If $S$ plays $h_1$, then $R$'s best response is to fold to all bets and call all checks ($g_1$). However $(h_1, g_1)$ is clearly not a Nash equilibrium of the game since $S$ has a different best response to $g_1$: bet every hand ($h_2$).

In considering $R$'s best response to $h_2$, we stumble across several interesting points. The first is that if $S$ bets every hand, i.e. the sender always sends the same signal regardless of the secret type, then $R$ cannot infer any information about the secret type from the signal. If this is the case, then $R$ has to make a guess about the sender's type $y$. For instance $R$ may guess that $S$ has a winning hand half the time, i.e. $P(y = \textbf{Winner}) = \frac{1}{2}$. Then $R$ could randomize uniformly between calling and folding ($g_2$). If the guess is correct then $(h_2, g_2)$ is a Nash equilibrium and the value of the game is $1/2$.

Poker is a great example of a mathematical signaling game played in real life. It has secret types, costly signaling, and plenty of deception.

\subsection{In Advertising}

In personalized advertising, advertisers tailor ad content for a specific consumer using that consumer's personal data. This is sometimes presented as a multiarmed bandit problem. We cast it as a signaling game.

We imagine ourselves in a common web scenario. A consumer visits a web page on which an advertiser may display an ad. The advertiser uses whatever data is available about the consumer to choose an ad. The consumer clicks on the ad if they find it appealing. In this game both sender and receiver have types: the sender's corresponding to the advertiser's products and the receiver's corresponding to the consumer's interests.

The advertiser wants brand exposure and, even better, for the consumer to click on the ad. The consumer, however, only wants to click on ads for products that suit their interests. This gives the advertiser an incentive to send ads which appeal to the consumer's type rather than represent their own type and creates an opportunity for deception.

Let us give a more formal statement. Let $U^D$ be the set of unit vectors in dimension $D$.
\begin{itemize}
    \item Sender type $y_S \in Y_S = U^D$.
    \item Receiver type $y_R \in Y_R = \mathbb{R}^D $.
    \item Signal space $X = \lbrace x_1, ..., x_N \vert x_i \in U^D \rbrace$.
    \item Action space $A = \lbrace 0,1 \rbrace$. 0 corresponds to no click, 1 to click.
    \item Utility function $\mu_S: Y_S \times X \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{align*}
        \mu_S = \langle y_S, x \rangle + a %\langle x, y_R \rangle
    \end{align*}
    \item Utility function $\mu_R: Y_S \times Y_R \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{align*}
        \mu_R = a \langle y_R, y_S \rangle
    \end{align*}
\end{itemize}

\noindent The twist in this game is that both $S$ and $R$ have types $y_S$ and $y_R$ respectively. These types are $D$-dimensional unit vectors. Each component of these vectors can be interpreted as an interest or product affinity. We can compare the types of $S$ and $R$, say via inner product, to determine the suitability of the advertiser's products for the consumer.

Furthermore, the ad $x$ is also a $D$-dimensional unit vector. Once again we can interpret the components as interests or product affinities and imagine that the ad promotes certain types of products. The action $a$ is binary, corresponding to whether or not the consumer clicks on the ad.

The advertiser's utility function $\mu_S$ is an example of costly signaling. It is more costly for an advertisers to send an ad farther from $y_S$. This punishes advertisers for deception, i.e. the advertiser incurs a cost as the signal $x$ diverges from $y_S$. This is similar to the job application game of Spence in which it was more costly for weak applicants to send strong applications, i.e. there was a signaling cost associated with misrepresentation of the sender's type. However in this case the cost is symmetric in that there are no weak and strong senders and all senders pay equally to diverge from their true type.

The utility function $\mu_S$ is motivated by actual advertising techniques. The first term describes inherent costs and benefits in showing ad $x$. The second term describes the reward if the consumer $R$ clicks. Let us examine the first term.

The least costly ad is $x = y_S$ with $\langle x, y_S \rangle = 1$. In our the ad interpretation, it perfectly represents the advertiser's products. However this utility value lines up with two other advertising ideas.
\begin{itemize}
\item The default ad should be the cheapest.
\item All other things equal, prefer the brand.
\end{itemize}
The ad $x = y_S$ is the most generic ad for advertiser with type $y_S$. It is therefore the cheapest ad to produce form the perspective of personalized advertising. It is the default ad. Second, this ad best represents the brand. Consistent brand identity is a big part of advertising and it is valued by advertisers. All other things being equal, the advertiser prefers to show the most brand-similar ad.

The $\langle x, y_S \rangle$ term reflects these inherent costs and benefits to showing ad $x$. The second term, $a$, is $1$ when $R$ clicks and $0$ otherwise. What may be surprisingly about this term is that the advertiser gets one unit of credit if $R$ clicks, regardless of $y_R$. There are many possible refinements here. A natural thought is that consumer may be worth more or less to an advertiser depending on the similarity of their types $\langle y_S, y_R \rangle$. Here we assume the advertiser is indifferent to who clicks on the ads so long as they do. 

The consumer's utility function $\mu_R$ rewards the receiver for clicking on ads for advertisers with suitable types. The intuition here is that ads are prevalent on the net and usually ignored. However if a consumer clicks on an ad, they will be diverted from their usual browsing activity onto another page.

In this game, we imagine the content of that page to be aligned with $y_S$, i.e. represent the advertisers' true type. However the consumer does not observe $y_S$ and so must to infer the advertiser's type given only $x$ and $y_R$. Clearly $R$ hopes to end up on a suitable page rather than an unsuitable one whenever $R$ opts to click.

There is also a subtle point about $R$'s actions $A=\lbrace 0, 1\rbrace$. To faithfully model the strategies available to $R$, we would need to give distributions over $A$ conditioned on the possible messages $x \in X$. However there are infinitely many possible messages since the messages can be any $D$-dimensional unit vector. To address this we define these distributions implicitly with threshold functions.

Since $\mu_R$ depends on $\langle y_R, y_S \rangle$, if $x \approx y_R$ and $x \approx y_S$ then $R$ should click on $x$. Also $x \approx y_R$ is some evidence that $x \approx y_S$ since it is costly for $R$ when $x$ departs from $y_S$. Therefore we define $R$'s strategies in terms of some threshold of the dot product $\langle y_R, x \rangle$. Since $y_R$ and $x$ are unit vectors, these are simply values in $[-1,1]$. Thus we amend the definition of the game so that we have $A = [0,1]$ but also $A_\theta = [\theta_1,...\theta_M]$ where each $\theta_j$ is a threshold value. These thresholds can be interpreted in advertising as how "clicky" the consumer is.
 
This game depicts a ubiquitous event in today's world: personalized advertising interaction. The game itself is best suited to be simulated as a repeated game among populations of advertisers and consumers. It is also not a zero-sum game. What types of equilibria can we hope to achieve in such games? We look to machine learning theory for ideas of how to proceed with our account of this matter.

\section{Learning and Games}

Learning theory is an appealing lens through which to examine games. It is natural to improve at a game as one plays and observes. Following this experience, we imagine the players of our signaling games as learning algorithms. The data these algorithms learn from is the history of games they have played. 

Specifically, we look to learning theory for direction in how players can learn to play equilibrium strategies in repeated signaling games. We begin with a review of online learning and learning with expert advice. This will bring us to regret minimization and how regret minimization can lead to equilibrium strategies in games. Finally we discuss a refinement of regret minimization suitable for signaling games and the types of equilibrium we can hope to achieve.

\subsection{Online Learning}

In the online learning setting, the learning algorithm is given one data point at a time over the course of $T$ rounds. Most importantly we make no distributional assumption about the data. This motivates a worst case adversarial analysis suited to the study of games. In each round $t$, the algorithm receives a point $x_t$ and predicts a label $\hat y_t$. The predicted label is compared with the true label $y_t$ and the algorithm incurs a loss according to the accuracy of its prediction.

We can concisely present the standard online learning setting as follows:

\begin{algorithm}[H]
\caption{Generic Online Learning}
\label{olalgo}
\begin{algorithmic}[1]
\For{$t=1$ to $T$}
    \State \text{receive } $x_t , y_t$
    \State \text{predict } $\hat{y}_t = h(x_t; w_t)$
    \State \text{incur loss } $L(\hat{y}_t, y_t)$
    \State \text{update state } $w_{t+1}$ from $w_t$
\EndFor
\State \Return $\textbf{w}_{T+1}$
\end{algorithmic}
\end{algorithm}
Our goal is to minimize the total loss $\mathcal{L}_T = \sum_{t=1}^T L(\hat{y}_t,y_t)$. The method to update the state $w_t+1$ after incuring the loss for a point $x$ is the critical part of an online learning algorithm.

\subsection{Learning with Expert Advice}

Learning with expert advice is a classic scenario in machine learning. For simplicity take the example of binary classification, i.e. we are given input ${x_t}$ and we must output a binary label $\hat{y}_t \in \lbrace 0,1 \rbrace$. We are given a set of $N$ experts $H = \lbrace h_1,...,h_N \rbrace $ who each predict a label $\lbrace \hat y_{t,1},...,\hat y_{t,N} \rbrace$. Our task is to accurately classify the inputs using predictions offered by the experts.

The experts need not be literal experts. In the case of games, we imagine the moves available to the players as our experts. A learning algorithm then produces a distribution overs the moves, i.e. a mixed strategy. As the number of rounds $T$ increases, we would like the mixed strategies produced by the algorithms to converge to equilibrium strategies. In our example of Simple Poker, which is zero-sum, we want these to be Nash equilibrium strategies.

\subsection{Regret}

Following our signaling game motivation, we focus on the adversarial case of online learning as opposed to the stochastic case. In the stochastic case we assume that all points $x$ are drawn independently from some data generating distribution $D_X$. In the study of games our data are the moves of other players, which are chosen strategically.

Instead of minimizing the total loss, we will compare our performance with that of the best expert in hindsight $h^*$, i.e. $h^* = \underset{i}{\text{min }} \sum_{t=1}^T L(\hat y_{t,i}, y_t)$. This is in the spirit of our worst-case adversarial analysis. In the stochastic case, we would like to bound the generalization error of the hypothesis our algorithms produces. However we do not have a notion of generalization in the adversarial case. We instead compare our performance to the best expert as a reference hypothesis. Our minimization objective then becomes
\begin{align*}
\mathcal{L}_T - \mathcal{L}_T^{\text{min}} = \sum_{t=1}^T L(\hat y_t, y_t) - \underset{i}{\text{min }} \sum_{t=1}^T L(\hat y_{t,i}, y_t)
\end{align*}
This quantity is called the external regret or simply regret $R_T$ after $T$ rounds. We are truly interested in the average regret per round $R_T/T$. If can we devise an algorithm with regret sublinear in $T$, then the average regret $R_T/T$ will asymptotically go to 0. Such an algorithm is said to have vanishing regret.

Critically, there is a connection between regret minimization and repeated games. First we consider the simplest case of the two-player zero-sum game, e.g. our game of simple poker. If a player plays using an external regret minimization algorithm with vanishing regret then that player's expected payoff per round will approach the value of the game, i.e. the minimax payoff (See \textbf{Theorem 7.2} in \cite{cesa2006prediction}).

\subsection{Weighted Majority}

The Weighted Majority algorithm \autoref{wmalgo} \cite{littlestone1994weighted} is a classic example of a regret minimization algorithm. Initially each expert is given equal weight. In each round we predict according to the weighted majority of the experts, e.g. in the first round this will be the usual majority. Whenever our prediction is incorrect, we reduce the weight of each expert who predicted incorrectly in that round. We do so by multiplying its current weight $w_{t,i}$ by some constant $\beta$ with $0 \le \beta < 1$. As an expert makes errors, its weight decreases and so its prediction counts less towards the ultimate prediction made by the algorithm.

\begin{algorithm}[H]
\caption{Weighted Majority}
\label{wmalgo}
\begin{algorithmic}[1]
\For{$i=1$ to $N$}
    \State{$w_{1,i} \leftarrow \frac{1}{N}$}
\EndFor
\For{$t=1$ to $T$}
    \State \text{receive } $x_t , y_t$
    \State $\hat y_t \leftarrow 1_{\sum_{i, y_{t,i}=1}^N w_t \ge \sum_{i, y_{t,i}=0}^N w_t}$
    \If{$\hat y_t \ne y_t$}
        \For{$i=1$ to $N$}
            \If{$y_{t,i} \ne y_t$}
                \State $w_{t+1,i} \leftarrow \beta w_{t+1,i} \quad\quad(0 \le \beta < 1)$
            \Else
                \State $w_{t+1,i} \leftarrow w_{t,i}$
            \EndIf
        \EndFor
    \EndIf
\EndFor
\State \Return $\textbf{w}_{T+1}$
\end{algorithmic}
\end{algorithm}

However, recall that we are focused on the adversarial case. This algorithm (or any deterministic algorithm, for that matter) will fare poorly against an adversary. To see this, suppose our adversary chooses data points for which the current weighted majority prediction will be wrong. We will predict incorrectly in every round. In the case of binary classification with binary loss our cumulative loss $\mathcal{L}_T = \sum_{t=1}^T R_t$ will be at least $T/2$ and so the algorithm has constant regret per round.

The Randomized Weighted Majority algorithm\autoref{rwmalgo} alleviates this by making probabilistic predictions. Instead of predicting according to the weighted majority, we sample from a distribution over the experts and predict according to the chosen expert. This distribution is determined by the experts' weights which are updated in the same fashion as the original Weighed Majority aglorhithm. Of course the adversary can still frustrate us to some degree but we shall see that this relatively simple modification to the algorithm produces a surprisingly promising result.

\begin{algorithm}[H]
\caption{Randomized Weighted Majority}
\label{rwmalgo}
\begin{algorithmic}[1]
\For{$i=1$ to $N$}
    \State{$w_{1,i} \leftarrow 1$}
    \State{$p_{1,i} \leftarrow \frac{1}{N}$}
\EndFor
\For{$t=1$ to $T$}
    \State \text{receive } $x_t, y_t$
    \For{$i=1$ to $N$}
        \If{$\hat y_{t,i} \ne y_t$}
            \State $w_{t+1,i} \leftarrow \beta w_{t+1,i} \quad\quad(0 \le \beta < 1)$
        \Else
            \State $w_{t+1,i} \leftarrow w_{t,i}$
        \EndIf
    \EndFor
    \State $W_{t+1} \leftarrow \sum_{i=1}^N w_{t+1}$
    \For{$i=1$ to $N$}
        \State $p_{t,i} = w_{t+1, i}/W_{t+1}$
    \EndFor
\EndFor
\State \Return $\textbf{w}_{T+1}$
\end{algorithmic}
\end{algorithm}
This algorithm obeys the following standard regret bound\cite{mohri2012foundations} for $\beta = \text{max}(\lbrace 1/2, 1-\sqrt{(\log N) T}\rbrace)$
\begin{align*}
R_T = \mathcal{L}_T - \mathcal{L}_T^{\text{min}} \le 2\sqrt{T \log N}
\end{align*}
To prove this, we consider the potential function
\begin{align*}
W_{t} = \sum_{i=1}^N w_{t,i}
\end{align*}
\noindent and give upper and lower bounds. For the upper bound, we note that $W_{t+1}$ can be decomposed into two sums: one of the weights of experts who predicted correctly in round $t$ and the other of $\beta$ times the weights of the experts who predicted incorrectly. Let $L_t$ be the loss in round $t$. With some manipulation we can express $W_{t+1}$ in terms of $W_{t}$ and $L_t$ as:
\begin{align*}
W_{t+1} = W_t(1-(1-\beta)L_t)
\end{align*}
\noindent Recall $w_{1,i} = 1$ for all $i$ and so $W_1 = N$. Therefore:
\begin{align*}
W_{T+1} = N \prod_{t=1}^T (1-(1-\beta)L_t)
\end{align*}
\noindent For the lower bound, we give an easy one: $W_{T+1} \ge \underset{i}{\text{max }} w_{T+1,i}=\beta^{\mathcal{L_T^\text{min}}}$, where $\mathcal{L_T^\text{min}}$ is the loss of the best single expert. In the case of binary classification and binary loss $\mathcal{L_T^\text{min}}$ is equal to the number of mistakes made by the best expert.

Now we compare our bounds:
\begin{align*}
    \beta^\mathcal{L_T^\text{min}} \le N \prod_{t=1}^T (1-(1-\beta)L_t)
\end{align*}
\noindent we take the $\log$ and apply the inequality $\log(1-x) \le -x$ valid for $x<1$:
\begin{align*}
    \mathcal{L_T^\text{min}}\log\beta & \le \log\left(N \prod_{t=1}^T (1-(1-\beta)L_t)\right)\\
    & \le \log N - (1-\beta)\sum_{t=1}^T L_t\\
    & = \log N - (1-\beta) \mathcal{L}_t
\end{align*}
\noindent A bit of algebra and a further application of the inequality allow us to conclude:
\begin{align*}
\mathcal{L}_T \le \frac{\log N}{1-\beta} + (2-\beta)\mathcal{L_T^\text{min}}
\end{align*}
\noindent In the case of the binary loss we know that $\mathcal{L_T^\text{min}} \le T$ so we can upper bound one more time:
\begin{align*}
\mathcal{L}_T \le \frac{\log N}{1-\beta} + (1-\beta)T + \mathcal{L_T^\text{min}}
\end{align*}
\noindent We differentiate this upper bound with respect to $\beta$ and set it to zero to find the optimal value. This gives us $\frac{\log N}{(1-\beta)^2} = 0$ which we can solve to find $\beta = \beta_0 = 1 - \sqrt{(\log N)\ T}$. Thus $\beta_0$ is the optimizing value if $1 - \sqrt{(\log N)\ T} \ge 1/2$ and $1/2$ is otherwise. The given bound is the result of the substitution of $\beta_0$ for $\beta$ in the last inequality.

This regret bound is sublinear in $T$ and so this algorithm has vanishing regret. As per the theorem from Cesa-Bianchi\cite{cesa2006prediction}, we may conclude that mixed strategies generated by Randomized Weighted Majority can approximate Nash equilibrium strategies in two-player zero-sum games given large enough $T$. While this is encouraging, it is not the case that a regret minimization algorithm can generate Nash equilibrium strategies for general games, e.g. non-zero-sum games. 

In fact finding Nash equilibrium in general games is known to be intractible (PPAD-complete\cite{daskalakis2009complexity}). PPAD problems are distinct from NP problems in that we always know that a solution exists but these solutions may be very hard to find. Specifically, PPAD problems are concerned with polynomial time verification of a binary relation $r(x,y)$ for which it is known that for all $x$ there exists a $y$ such that $(x,y)$ holds. For example we always know that a Nash equilibrium exists for any given game. A related example of a PPAD problem is finding a fixed point of a continuous function over a compact convex set, as in Brouwer's fixed point theorem which inspired Nash.

\section{Correlated Equilibrium}

Signaling games are not necessarily zero sum and so Nash equilibrium is not a computationally attractive solution. Instead we turn to another sort of equilibrium called correlated equilibrium. Originally introduced by Aumann \cite{aumann1974subjectivity}\cite{aumann1987correlated}, correlated equilibrium is different from Nash in that it specifies a joint distribution over the actions of all players rather than individual distributions over the actions of each player. Let $H$ be the set of mixed strategies for $S$ and $G$ the set of mixed strategies for $R$. A correlated equilibrium is a joint distribution $p$ over $G \times H$ such that for all $h\in H$ with $p(h) > 0$ and all $h^\prime \in H$
\begin{align*}
\sum_{g \in G} p(h,g)\mu_S(h, g) \ge \sum_{g \in G} p(h^\prime,g)\mu_S(h^\prime, g)
\end{align*}
\noindent and similarly for $R$
\begin{align*}
\sum_{h \in H} p(h,g)\mu_R(h, g) \ge \sum_{h \in H} p(h,g^\prime)\mu_R(h, g^\prime)
\end{align*}

The intuition behind correlated equilibrium is that players may rationally play strategies besides Nash equilibrium strategies when all players observe the value of a random variable, sometimes called a correlation device. The device chooses joint strategy assignments for each player with some probability and informs each player of their chosen strategy. If no player has an incentive to deviate from the assigned strategy (considering the known distribution over joint strategy assignments) then the joint distribution is a correlated equilibrium.

However an explicit correlation device is not necessary to achieve correlated equilibrium in repeated games. Foster and Vohra \cite{foster1997calibrated} show that players who play myopic best responses to calibrated forecasts of the other player's strategies will converge to a correlated equilibrium. A calibrated forecast and the concept of calibration come from statistics. It roughly means that players assign probabilities to each other's actions that coincide with the actions' empirical probabilities in the limit. Here the history of plays can be viewed as an implicit correlation device.

This is plausible in advertising. A consumer's history of ad experiences on the internet certainly inform his future potential ad interaction. The salient bad practices of so many internet advertisers comes to mind, e.g. the demand for spam block and ad block. 

\subsection{Internal Regret and Swap Regret}

As external regret minimization algorithms achieve Nash equilibrium in zero-sum games, swap regret minimization algorithms achieve correlated equilibrium in general games \cite{blum2007external}. To begin we introduce the concept of \emph{internal regret} \cite{foster1998asymptotic}. 
\begin{align*}
\mathcal{L_T} - \underset{f \in C_{j,k}}{\text{min }}\mathcal{L}_T^{j\rightarrow k}
\end{align*}
Instead of using the best single expert in hindsight as our reference, we compare our performance to the ways in which we could have exchanged a single action for another. $C_{j,k}$ is the set of functions $f: A \rightarrow A$ that perform this single action exchange.

Hart and Mas-Colell\cite{hart2000simple} give a method for playing repeated games in which the players update their strategies in proportion to internal regret. At each step $t$ each player sticks with the last move $j$ or switches to a different one $k$ with some probability. We compare the cumulative loss of the sequence formed by replacing $j$ with $k$ in a player's sequence of moves thus far with the actual cumulative loss $\mathcal{L}_t$.

Call this alternative loss $\mathcal{L}_t^{j\rightarrow k}$. As we wish to minimize internal regret, the only moves with positive probabilities are those with $\mathcal{L}_t^{j\rightarrow k} < \mathcal{L}_t$. The probability on move $k$ is then proportional to the internal regret for playing $j$ instead of $k$, $\mathcal{L}_t - \mathcal{L}_t^{j\rightarrow k}$. The last strategy $j$ always retains some probability mass as well.

\emph{Swap regret} considers all ways in which actions could be mapped between themselves, i.e. where we can swap as many actions as we please.
\begin{align*}
R_T^{\text{swap}} = \mathcal{L_T} - \underset{f \in C}{\text{min }}\mathcal{L}_T^{\text{swap}}\end{align*}
Here the set $C$ contains all functions $f: [1,...,N] \rightarrow [1,...,N]$ when there are $N$ experts. As we imagine the actions or experts as moves in a game, having zero swap regret implies no strategic modifications would be advantageous. The joint distribution over the players' actions is a correlated equilibrium if every player would have 0 swap regret playing it. Players minimizing swap regret will thereby converge to correlated equilibrium.

\subsection{Swap Regret Minimization}

Blum and Mansour\cite{blum2007external} prove this and conveniently give us a way to generate an algorithm with vanishing swap regret given an algorithm with vanishing external regret. Given such a regret minimization algorithm, say Randomized Weighted Majority, we instantiate $N$ copies, one for each expert or pure strategy. At each step each instance $i$ generates a distribution $q_i$ which will form a component of the overall mixed strategy.

At each time step $t$ we form the stochastic matrix $Q_t$, where row $i$ equals $q_i$. We then find a vector $p_t$ such that $p_t Q_t = p_t$, i.e. $p_t$ is the corresponding stationary distribution of the stochastic matrix. This distribution $p_t$ can be interpreted in two ways. Most directly, it can be seen as choosing action $j$ with probability $p_j$. However it can also be interpreted as selecting sub algorithm $j$ with probability $p_j$ and then using that algorithm to select the action. 

When the algorithm receives a loss vector $L_t$, i.e. an $N$-dimensional vector of losses for each action, it is partitioned among the $N$ instances. Each instance $i$ receives a fraction of the loss $p_{t,i} L_t$ so the expected loss $L_{i,t}$ for algorithm $i$ is $p_{t,i} (q_{t,i} \cdot L_t)$. 
Each instance has vanishing external regret by assumption. This means that for any $i,j$:
\begin{align*}
\sum_{t=1}^T p_{t,i} (q_{t,i}\cdot L_t) \le \sum_{t=1}^T p_{t,i} L_{t,j} + R_{T,i}
\end{align*}
\noindent where $R_{T,i}$ is the regret bound we have for instance $i$. If we sum the losses over all algorithms at any time $t$ we will recover the original loss $L_t$ since get $\sum_{i=1}^N p_{t,i}(q_{t,i}\cdot L_t) = p_t Q_t L_t = p_t L_t$ as $p_t Q_t = p_t$ by design. Thus summing over the left hand side of inequality yields $\mathcal{L}_T$, the cumulative loss of the algorithm. Since the inequality holds for all $j$, we have that for any function $f : [1,...,N] \rightarrow [1,...,N]$
\begin{align*}
    \mathcal{L}_T & \le \sum_{i=1}^N\sum_{t=1}^T p_{t,i} L_{t,f(i)} + \sum_{i=1}^N R_{T,i}\\
    & = \mathcal{L}_T^{\text{swap}} + \sum_{i=1}^N R_{T,i}\\
    & \implies R_T^{\text{swap}} = \mathcal{L_T} - \mathcal{L}_T^{\text{swap}} \le \sum_{i=1}^N R_{T,i}
\end{align*}

The main idea of this proof is that instance $i$ will provide low regret with respect to swaps for action $i$ , i.e. of functions $f_{i,j} \in C_{i,j}$. Finally we are able to exploit the underlying external regret guarantees (e.g. $2\sqrt{T \log N}$ for Randomized Weighted Majority) for each instance to give a swap regret bound

\begin{align*}
R_T^{\text{swap}} = \mathcal{L}_T - \mathcal{L}_T^{\text{swap}} \le c N\sqrt{T \log N}
\end{align*}

with some constant $c$. Recall that for Randomized Weighted Majority, we have that the external regret $R_T \le 2\sqrt{T \log N}$. The great part about this bound is that the dependence on $N$ is only logarithmic and so we are relatively free to consider a very large numbers of expert. In the case of swap regret we cannot maintain this luxury and suffer a linear dependence on $N$. While we are able to apply swap regret minimization to a broader class of games, we have a different type of restriction on the complexity of the games. It is possible to achieve an $O(\sqrt{N T \log N})$ swap regret bound but that is not much next to the $\log N$ external regret bound.

\subsection{Bandit Setting}

A final and necessary distinction is that between the partial information or bandit setting and the full information setting. In the proof of the swap regret bound, the algorithm received a loss vector $L_t$ containing a loss for each of the $N$ moves. This is the full information setting and we update every value of the matrix $Q_t$ each round. In the bandit setting, the algorithm only receives loss for the move it actually played.

Suppose the algorithm plays action $j$ in round $t$. In place of a loss vector $L_t$, the algorithm receives a pair $(j, l_{t,j})$. Each instance $i$ then receives a pair $(j, g_{t,j})$ where $g_{t,j} = p_{t,i}l_{t,j}q_{t,i,j}/p_{t,j}$. This is the probability the overall algorithm plays move $i$ times the loss for move $j$ times the probability instance $i$ plays move $j$ divided by the probability that the overall algorithm plays $j$. The proof goes through similarly as before in that the swap regret of the overall algorithm is bounded by the sum of the external regret bounds of the instance algorithms.

The bandit setting is important to consider in games because it allows one to study in which the players only know their own payoff functions but not those of the other players. A version of this is examined in \cite{kalai1993rational} where the authors show that players can learn to play Nash equilibrium without knowledge of the other players' payoff functions. We will tackle our advertising-inspired signaling game in the bandit setting as well.

\section{Swap Regret Minimization in Signaling Games}

Finally we return to our ad game, armed with the theory of online learning and a swap regret minimizing algorithm. Our goal here is to present our ad game as a game played by online learning algorithms. In particular, we model our players swap regret minimization algorithms using Randomized Weighted Majority as the subroutine. For convenience, here is our game again:
\begin{itemize}
    \item Sender type $y_S \in Y_S = U^D$.
    \item Receiver type $y_R \in Y_R = \mathbb{R}^D $.
    \item Signal space $X = \lbrace x_1, ..., x_N \vert x_i \in U^D \rbrace$.
    \item Action space $A = \lbrace 0,1 \rbrace$ with thresholds $A_\theta = \lbrace \theta_1,...,\theta_M \rbrace$, $\theta_j \in [-1,1]$.
    \item Utility function $\mu_S: Y_S \times X \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{align*}
        \mu_S = \langle y_S, x \rangle + a% \langle y_R, x \rangle
    \end{align*}
    \item Utility function $\mu_R: Y_S \times Y_R \times A \rightarrow \mathbb{R}$ is defined as:
    \begin{align*}
        \mu_R = a \langle y_R, y_S \rangle% + \langle y_R, x \rangle
    \end{align*}
\end{itemize}

In order to apply the algorithm here we need to make a few adjustments. First, we need to modify the Randomized Weighted Majority algorithm to work with losses in $[0,1]$ rather than the binary loss. To do so we replace the weight update step
\begin{algorithmic}
\If{$\hat y_{t,i} \ne y_t$}
    \State $w_{t+1,i} \leftarrow \beta w_{t+1,i} \quad\quad(0 \le \beta < 1)$
\Else
    \State $w_{t+1,i} \leftarrow w_{t,i}$
\EndIf
\end{algorithmic}
with the following:
\begin{algorithmic}
\State $w_{t+1,i} \leftarrow w_{t+1,i} \exp(-\eta L_{t,i}) \quad\quad \eta = O\left(1/\sqrt{t}\right)$
\end{algorithmic}
Instead of being multiplied by a constant $\beta$, the weights are reduced in proportion to the loss they incur. This modification applied to the Weighted Majority Algorithm yields the Exponential Weighted Average algorithm, which has similar properties. Note that when $L_{t,i} = 0$, i.e. no loss is suffered, then the weights remain unchanged just as in the previous update. Finally we transform the utility functions into loss functions to be minimized since they are originally given as utilities to be maximized. First we apply a linear transform to scale their values to $[0,1]$ and then subtract them from 1.
The complete simulation procedure is as follows.
\begin{algorithmic}[1]
\State{$p_{1}^S \leftarrow \frac{1}{N}\textbf{1}$}
\State{$p_{1}^R \leftarrow \frac{1}{N}\textbf{1}$}
\For{$i=1$ to $N$}
    \State{$W_{1,i}^S \leftarrow \textbf{1}$}
    \State{$Q_{1,i}^S \leftarrow \frac{1}{N}\textbf{1}$}
\EndFor
\For{$i=1$ to $M$}
    \State{$W_{1,i}^R \leftarrow \textbf{1}$}
    \State{$Q_{1,i}^R \leftarrow \frac{1}{N}\textbf{1}$}
\EndFor
\end{algorithmic}
First we initialize swap regret minimization algorithms for the players $S$ with $N$ ads and $R$ with $M$ thresholds. Initially each player plays the uniform distribution over his moves and all instance weights are $1$. The rows of $N\times N$ matrix $Q_t^S$ and and $M\times M$ matrix $Q_t^R$ represent the probability distributions maintained by the Randomized Weighted Majority instances. The $W$ matrices represent the weights of the instances. Then in each round $t=1$ up to $T$ we do:
\begin{algorithmic}
\State $x_t \leftarrow \texttt{Sample}(p_t^S)$
\State $\theta_t \leftarrow \texttt{Sample}(p_t^R)$
\State $a_t \leftarrow 1_{\langle x, y_R \rangle > \theta_t}$
\State $L_t^S \leftarrow \mu_S(y_S,x_t,a_t)$
\State $L_t^R \leftarrow \mu_R(y_S,y_R,a_t)$
\end{algorithmic}
First we sample from each player's distribution to generate a move. We test if the ad $x$ meets $R$'s threshold for clicking and calculate the utility for the players. To update the distribution $p_{t}^S\rightarrow p_{t+1}^S$, we update first the stochastic $Q$ matrices. Suppose $x_t = j$:
\begin{algorithmic}
\For{$i=1$ to $N$}
    \State $l_{t,i}^S = p_{t,i}^S L_t^S Q_{t,i,j}^S / p_{t,j}^S$
    \State $W_{t+1,i,j}^S \leftarrow W_{t,i,j}^S\exp(-\eta l_{t,i}^S) \quad\quad \eta = O(1\sqrt{t})$
    \State $Q_{t+1,i}^S \leftarrow \texttt{Normalize}(W_{t+1,i}^S)$
\EndFor
\State $p_{t+1}^S \leftarrow \texttt{Stationary}(Q_{t+1}^S)$
\end{algorithmic}
We can find the stationary distribution $p$ associated right stochastic matrix $Q$ by finding a left eigenvector of $Q$ with eigenvalue $1$. The procedure is symmetric for $R$. For the sake of completeness we also present the procedure in its entirety \autoref{adgamesim}.
\begin{algorithm}
\caption{Ad Game Simulation Procedure}
\label{adgamesim}
\begin{algorithmic}[1]
\State{$p_{1}^S \leftarrow \frac{1}{N}\textbf{1}$}
\State{$p_{1}^R \leftarrow \frac{1}{N}\textbf{1}$}
\For{$i=1$ to $N$}
    \State{$W_{1,i}^S \leftarrow \textbf{1}$}
    \State{$Q_{1,i}^S \leftarrow \frac{1}{N}\textbf{1}$}
\EndFor
\For{$i=1$ to $M$}
    \State{$W_{1,i}^R \leftarrow \textbf{1}$}
    \State{$Q_{1,i}^R \leftarrow \frac{1}{N}\textbf{1}$}
\EndFor
    \For{$t=1$ to $T$}
    \State $x_t \leftarrow \texttt{Sample}(p_t^S)$
    \State $\theta_t \leftarrow \texttt{Sample}(p_t^R)$
    \State $a_t \leftarrow 1_{\langle x, y_R \rangle > \theta_t}$
    \State $L_t^S \leftarrow \mu_S(y_S,x_t,a_t)$
    \State $L_t^R \leftarrow \mu_R(y_S,y_R,a_t)$
    \For{$i=1$ to $N$}
        \State $l_{t,i}^S = p_{t,i}^S L_t^S Q_{t,i,j}^S / p_{t,j}^S$
        \State $W_{t+1,i,j}^S \leftarrow W_{t,i,j}^S\exp(-\eta l_{t,i}^S)$
        \State $Q_{t+1,i}^S \leftarrow \texttt{Normalize}(W_{t+1,i}^S)$
    \EndFor
    \For{$i=1$ to $M$}
        \State $l_{t,i}^R = p_{t,i}^R L_t^R Q_{t,i,j}^R / p_{t,j}^R$
        \State $W_{t+1,i,j}^R \leftarrow W_{t,i,j}^R\exp(-\eta l_{t,i}^R)$
        \State $Q_{t+1,i}^R \leftarrow \texttt{Normalize}(W_{t+1,i}^R)$
    \EndFor
    \State $p_{t+1}^S \leftarrow \texttt{Stationary}(Q_{t+1}^S)$
    \State $p_{t+1}^R \leftarrow \texttt{Stationary}(Q_{t+1}^R)$
\EndFor
\State \Return $p_{T+1}^S, p_{T+1}^R$
\end{algorithmic}
\end{algorithm}
To play with a population of advertisers and consumers, we could simply sample each player from their respective population at the beginning of each round.

\subsection{Simulation Results}

\subsection{Conclusion}
This paper gives an account of advertising from the perspective of game theory and machine learning. It suggests the signaling game as an appropriate model for advertising interaction and online learning as an appealing approach to player behavior. It shows that it is possible to efficiently compute correlated equilibria in multiplayer signaling games using a flavor of regret minimization algorithms. This paper provides direction for future exploration of signaling games and online learning in advertising.

\bibliographystyle{amsplain}
\bibliography{dk_thesis}

\end{document}
